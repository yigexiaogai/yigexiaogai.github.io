<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Gai&#39;s blog</title>
  
  <subtitle>show my life</subtitle>
  <link href="https://yigexiaogai.github.io/atom.xml" rel="self"/>
  
  <link href="https://yigexiaogai.github.io/"/>
  <updated>2022-06-19T08:59:04.783Z</updated>
  <id>https://yigexiaogai.github.io/</id>
  
  <author>
    <name>Gai</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>小该碎碎念220618</title>
    <link href="https://yigexiaogai.github.io/2022/06/18/%E5%B0%8F%E8%AF%A5%E7%A2%8E%E7%A2%8E%E5%BF%B5220618/"/>
    <id>https://yigexiaogai.github.io/2022/06/18/%E5%B0%8F%E8%AF%A5%E7%A2%8E%E7%A2%8E%E5%BF%B5220618/</id>
    <published>2022-06-18T15:41:36.000Z</published>
    <updated>2022-06-19T08:59:04.783Z</updated>
    
    <content type="html"><![CDATA[    <div id="aplayer-frzNkmDu" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="28285910" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#3F51B5"></div><center><blockquote><p>时间是让人猝不及防的东西，晴时有风阴有时雨<br>争不过朝夕又念着往昔，偷走了青丝却留住一个你</p></blockquote><center></center></center>]]></content>
    
    
    <summary type="html">毕业了！</summary>
    
    
    
    <category term="日记" scheme="https://yigexiaogai.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
    <category term="小该碎碎念" scheme="https://yigexiaogai.github.io/tags/%E5%B0%8F%E8%AF%A5%E7%A2%8E%E7%A2%8E%E5%BF%B5/"/>
    
  </entry>
  
  <entry>
    <title>小该碎碎念220609</title>
    <link href="https://yigexiaogai.github.io/2022/06/09/%E5%B0%8F%E8%AF%A5%E7%A2%8E%E7%A2%8E%E5%BF%B5220609/"/>
    <id>https://yigexiaogai.github.io/2022/06/09/%E5%B0%8F%E8%AF%A5%E7%A2%8E%E7%A2%8E%E5%BF%B5220609/</id>
    <published>2022-06-09T15:44:01.000Z</published>
    <updated>2022-06-09T17:46:02.597Z</updated>
    
    <content type="html"><![CDATA[<p>2022年6月9日，晴，虽然晴，但是很闷热。<br>马上就要到梅雨季节了，接下来应该会下雨下个不停。我的心情有点像即将到来的梅雨季——常常阴霾，淅淅沥沥。为什么呢？是因为即将毕业了吗？是因为对未来出国留学的不确定性而感到担忧吗？是四个月前死去的回忆突然开始攻击我了吗？我不知道，也许都有，也许都没有。</p><p>最近我睡得很晚，基本在凌晨两三点之间合眼，当然这和我假期养成的坏习惯有关系。其实我也很想早睡，但是睡不着的时候，总有无数的思绪涌上心头，就像晕车后迫不及待要从胃经过食道再逃出喉咙的呕吐物一样，让人烦躁、恶心。我尝试用听歌来转移难受，结果听得最多的是张惠妹，是《如果你也听说》、《我要快乐》、《原来你什么都不要》、《记得》……太悲了。</p><p>睡觉外的时间我也不能闲下来，一旦手边空荡荡的，悲伤、焦虑、恐惧等等负面情绪就会充斥着我的内心。因此我希望能够不停地做些事情，有意义或无意义都无所谓，美其名曰享受生活，但实际上我知道的，这是一种麻醉自我的手段。最近这些天，我们寝室和隔壁寝室会组队打三国杀，大约从晚上七点开始，一玩就是四五个小时。虽然说，大家没心没肺的叫着、笑着，为了揣测对方的身份“勾心斗角”，为了获得最终的胜利“尔虞我诈”，确实很快乐。哦今天晚上我们寝室还去唱K了，隔壁寝室的两位的大聪明因为没做核酸所以去不了。我们唱到声音嘶哑，精疲力竭，大汗淋漓，真的很快乐。可这样的快乐，好像只能持续到15号的毕业典礼……</p><p>我知道的，天下没有不散的宴席，没有永远的快乐，我知道的。</p><p>最近我拟定了一个计划——在离开中国前，约一约好久不见的以前的好友们。一件事情让我产生了这个想法。</p><p>最近有一个两年不见的老友来见我了，她是曾经特别吸引我的一位姑娘，因为种种原因，我们大吵了一架，她删了我的好友。虽然后来机缘巧合下我又加回了她的微信，但也没有再说过一句话。我能看到她的朋友圈，我知道她住院了；我看到学院发的各种名单，都没有她的名字，我知道她留级了。我无视了。</p><p>两周前，她约我出来吃饭，告诉了我这一年发生在她身上的事情：小时候她的脑血管落下的病根，在一个凌晨突然发作，伴随着巨大的疼痛，她失语、身体麻痹，甚至无法喊出“救命”，她不断地敲击床板，从上铺翻滚到地下让室友发现；那天晚上，她被送到最近的医院抢救，从鬼门关硬生生爬了回来；之后她又不断辗转大大小小的医院进行康复，因为这场灾难给她留下了后遗症，她无法用右手进行精细的操作，人也常常会感到恶心想吐。</p><p>她说约我出来是为了和过去的自己和解，以及从我身上找到积极快乐的情绪，因为病魔带给了她的家庭和人生巨大的坎坷，但同时她也害怕这些事会带给我巨大的负能量。听了这些的我一时语塞，心中充满了愧疚。如果我能放下无谓的自尊，如果我能在她生病的时候找她一次，哪怕是线上的一句慰问……差一点，我差一点永远失去了一个朋友，那么轻易，那么残酷。这些无法预料的不测，让我想起了其他不曾联系的朋友们，也许“永别”比想象的要“轻描淡写”。当然，她与我之间情况还没那么糟，她所需要的积极向上的情绪，正是我的机会，赎罪的机会。</p><p>虽然说，我自己的内心也常如破碎的泡影，更无法确信明天一定是美好的，但这个世界上总有更加艰难的人，学会了硬着头皮把人生走下去。</p>]]></content>
    
    
    <summary type="html">我的新栏目兼日记，记录一些生活中的事情与感悟。这是第一期。</summary>
    
    
    
    <category term="日记" scheme="https://yigexiaogai.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
    <category term="小该碎碎念" scheme="https://yigexiaogai.github.io/tags/%E5%B0%8F%E8%AF%A5%E7%A2%8E%E7%A2%8E%E5%BF%B5/"/>
    
  </entry>
  
  <entry>
    <title>评论与实时聊天系统</title>
    <link href="https://yigexiaogai.github.io/2022/06/04/%E8%AF%84%E8%AE%BA%E4%B8%8E%E5%AE%9E%E6%97%B6%E8%81%8A%E5%A4%A9%E7%B3%BB%E7%BB%9F/"/>
    <id>https://yigexiaogai.github.io/2022/06/04/%E8%AF%84%E8%AE%BA%E4%B8%8E%E5%AE%9E%E6%97%B6%E8%81%8A%E5%A4%A9%E7%B3%BB%E7%BB%9F/</id>
    <published>2022-06-04T14:08:23.000Z</published>
    <updated>2022-06-04T15:55:40.347Z</updated>
    
    <content type="html"><![CDATA[<p>ohhhhhhh！经过整整两天的努力，本博客终于上线了评论和在线聊天系统辣！！！先小夸一下自己。</p><img src="/2022/06/04/%E8%AF%84%E8%AE%BA%E4%B8%8E%E5%AE%9E%E6%97%B6%E8%81%8A%E5%A4%A9%E7%B3%BB%E7%BB%9F/1.jpg" class><h1>前言</h1><p>评论和聊天系统的获取主要参考了<a href="https://butterfly.js.org/posts/ceeb73f/">butterfly的官方文档</a>的<strong>主题配置-2</strong>，里面有对应的内容板块。<strong>但！是！如果事情真的有那么简单就好了！</strong> 由于所用到的系统皆为第三方工具，官方文档会让你到对应网站查看API和使用说明，其中的步骤多如牛毛、遇到的问题层出不穷（哭），这也是我花了将近两天的原因。总之，<strong>本文仅仅解释这两个系统在博客中的使用方法，任何有关“获取”的内容将会放在以后的关于hexo美化的博文中。</strong> 让我们开始吧。</p><h1>评论系统</h1><p>评论系统如图所示（虽然你把页面拉到最底部也能看到）</p><img src="/2022/06/04/%E8%AF%84%E8%AE%BA%E4%B8%8E%E5%AE%9E%E6%97%B6%E8%81%8A%E5%A4%A9%E7%B3%BB%E7%BB%9F/2.png" class><p>该评论系统基于<strong>waline</strong>，是一个带有后端的评论与浏览量服务系统，这是它的<strong>官方文档-&gt;</strong> <a href="https://waline.js.org/guide/get-started.html">戳我</a>。</p><h2 id="基本介绍">基本介绍</h2><p>抬头处有昵称、邮箱、网址三个选填项。</p><ul><li>如果不填昵称，那么评论时昵称默认为“匿名”。</li><li>填写邮箱后，如果作者回复了你的评论，你就可以在对应邮箱中收到回复的提醒。</li><li>填写网址后，点击评论旁用户的昵称便可以跳转到相应网址（即使你没有个人博客，填个B站主页也可以）</li></ul><img src="/2022/06/04/%E8%AF%84%E8%AE%BA%E4%B8%8E%E5%AE%9E%E6%97%B6%E8%81%8A%E5%A4%A9%E7%B3%BB%E7%BB%9F/3.png" class><p>评论框中的文本格式支持markdown语句，因此不仅仅能发普通评论，也可以发公式，代码，链接等等markdown格式文本。<br>左下第一个button是markdown的语法说明，第二个是表情包（成功调用这个我真的是花了巨巨巨久时间），第三个是图片，第四个是评论预览。<br>在你发布评论后，我会第一时间通过邮箱收到你的评论。</p><h2 id="登录">登录</h2><p>右下方有一个登录选项，你也可以选择登录，注册。在注册的过程中会让你输入<strong>邮箱</strong>和<strong>个人网站</strong>，这就等同于上面的<strong>邮箱</strong>和<strong>网址</strong>，其中注册时，邮箱必填，个人网站选填。<strong>填写qq邮箱时，系统会自动获取你的qq头像作为用户头像哦！</strong></p><div align="center"><img src="/2022/06/04/%E8%AF%84%E8%AE%BA%E4%B8%8E%E5%AE%9E%E6%97%B6%E8%81%8A%E5%A4%A9%E7%B3%BB%E7%BB%9F/4.png" width="30%"></div><p>注册完后，你就会成为本博客后端服务器中的用户。<strong>注意：只是在本博客中有该账号，同样用了waline的其他博客是不相通的哦！</strong> 打个比方，bilibili和微博用了同一款评论服务，但你还是得有两个号。</p><p>登陆后如图所示，点击评论框旁头像下方的昵称就可以进入个人页面了。<strong>P.S. 暂时没有找到注销账号的方法，我一个管理员都不行。</strong></p><img src="/2022/06/04/%E8%AF%84%E8%AE%BA%E4%B8%8E%E5%AE%9E%E6%97%B6%E8%81%8A%E5%A4%A9%E7%B3%BB%E7%BB%9F/5.png" class><h1>实时聊天系统</h1><p>评论系统需要点击右下方中的一个带有“SMS”字样的小工具弹出，界面如图所示。</p><div align="center"><img src="/2022/06/04/%E8%AF%84%E8%AE%BA%E4%B8%8E%E5%AE%9E%E6%97%B6%E8%81%8A%E5%A4%A9%E7%B3%BB%E7%BB%9F/6.png" width="30%"></div><h2 id="基本介绍-2">基本介绍</h2><p>右下方的第一个回形针按钮代表附件，似乎可以通过该按钮发送文件（例如txt文件），但是好像只能打开，无法下载，总之功能不稳定，建议使用邮箱代替文件传送。同样可以发送表情包。</p><p><strong>注意：由于信息需要按回车发送，所以，1.信息中不会存在换行符，但使用附件可以规避这一点。2.手机客户端上不能发送消息，因为手机输入法一般只有换行符，没有回车键，除非你的输入法可以设置。</strong></p><h1>welcome</h1><p>最后，欢迎来到我的博客，你可以在这里畅所欲言哦！<strong>Peace!</strong></p><img src="/2022/06/04/%E8%AF%84%E8%AE%BA%E4%B8%8E%E5%AE%9E%E6%97%B6%E8%81%8A%E5%A4%A9%E7%B3%BB%E7%BB%9F/7.gif" class>]]></content>
    
    
    <summary type="html">主要解释了本博客中评论与聊天系统的使用方法。</summary>
    
    
    
    <category term="hexo" scheme="https://yigexiaogai.github.io/categories/hexo/"/>
    
    
    <category term="hexo相关, hexo美化" scheme="https://yigexiaogai.github.io/tags/hexo%E7%9B%B8%E5%85%B3-hexo%E7%BE%8E%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战（五·续一）</title>
    <link href="https://yigexiaogai.github.io/2022/04/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%C2%B7%E7%BB%AD%E4%B8%80%EF%BC%89/"/>
    <id>https://yigexiaogai.github.io/2022/04/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%C2%B7%E7%BB%AD%E4%B8%80%EF%BC%89/</id>
    <published>2022-04-26T08:57:30.000Z</published>
    <updated>2022-06-09T15:40:30.128Z</updated>
    
    <content type="html"><![CDATA[<h1>用keras单隐层网络预测客户流失率</h1><p><strong>这一节的实例过程中出现的结果，每个人之间可能都有较大出入。大家主要还是要理解代码的思路！另外下载教学资源中，此节的数据集和kaggle上作者上传的数据有所差别，强烈建议下载kaggle上的数据集：<a href="https://www.kaggle.com/datasets/tohuangjia/bank-customer">点击此处</a></strong></p><h2 id="数据准备与分析">数据准备与分析</h2><p>打开数据集观察一下，会发现具体包含以下信息：</p><ul><li>Name: 客户姓名</li><li>Gender: 性别</li><li>Age: 年龄</li><li>City: 城市</li><li>Tenure: 已经成为客户的年头</li><li>ProductsNo: 拥有的产品数量</li><li>HasCard: 是否有信用卡</li><li>ActiveMember: 是否为活跃用户</li><li>Credit: 信用评级</li><li>AccountBal: 银行存款余额</li><li>Salary: 薪水</li><li>Exited: 客户是否已经流失</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df_bank = pd.read_csv(<span class="string">&quot;/content/drive/MyDrive/Colab Notebooks/Learning ML from 0/dataset/BankCustomer.csv&quot;</span>)</span><br><span class="line">df_bank.head()</span><br></pre></td></tr></table></figure><img src="/2022/04/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%C2%B7%E7%BB%AD%E4%B8%80%EF%BC%89/1.png" class><p>显示一下数据的分布情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="comment"># 显示不同特征的分布情况</span></span><br><span class="line">features = [<span class="string">&#x27;City&#x27;</span>, <span class="string">&#x27;Gender&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Tenure&#x27;</span>, <span class="string">&#x27;ProductsNo&#x27;</span>, <span class="string">&#x27;HasCard&#x27;</span>, <span class="string">&#x27;ActiveMember&#x27;</span>, <span class="string">&#x27;Exited&#x27;</span>]</span><br><span class="line">fig = plt.subplots(figsize=(<span class="number">15</span>, <span class="number">15</span>))</span><br><span class="line"><span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(features):</span><br><span class="line">    plt.subplot(<span class="number">4</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.subplots_adjust(hspace=<span class="number">1.0</span>)</span><br><span class="line">    sns.countplot(x=j, data=df_bank)</span><br><span class="line">    plt.title(<span class="string">&#x27;No. of costumers&#x27;</span>)</span><br></pre></td></tr></table></figure><img src="/2022/04/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%C2%B7%E7%BB%AD%E4%B8%80%EF%BC%89/2.png" class><p>从图中看出，北京客户最多，男女比例基本一致，年龄和客户呈正态分布。对这个数据集需要做以下几个方面的清理工作：</p><ul><li>（1）性别。这是一个二元类别特征，需要转化成1和0。</li><li>（2）城市。这是一个多元类别特征，应该转换成多个二元类别哑变量。</li><li>（3）姓名这个字段和客户流失与否无关，可以忽略。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把二元类别文本数字化</span></span><br><span class="line">df_bank[<span class="string">&#x27;Gender&#x27;</span>].replace(<span class="string">&quot;Female&quot;</span>, <span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df_bank[<span class="string">&#x27;Gender&#x27;</span>].replace(<span class="string">&quot;Male&quot;</span>, <span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 显示数字类别</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Gender unique values&quot;</span>, df_bank[<span class="string">&#x27;Gender&#x27;</span>].unique())</span><br><span class="line"><span class="comment"># 把多元类别转换成多个二元类别哑变量，然后放回原始数据集</span></span><br><span class="line">d_city = pd.get_dummies(df_bank[<span class="string">&#x27;City&#x27;</span>], prefix=<span class="string">&#x27;City&#x27;</span>)</span><br><span class="line">df_bank = [df_bank, d_city]</span><br><span class="line">df_bank = pd.concat(df_bank, axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 构建标签和特征集</span></span><br><span class="line">y = df_bank[<span class="string">&#x27;Exited&#x27;</span>]</span><br><span class="line">X = df_bank.drop([<span class="string">&#x27;Name&#x27;</span>, <span class="string">&#x27;Exited&#x27;</span>, <span class="string">&#x27;City&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">X.head()</span><br></pre></td></tr></table></figure><p>输出的清理之后的数据集如下图所示。此时新数据集的特征数目是12个。</p><img src="/2022/04/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%C2%B7%E7%BB%AD%E4%B8%80%EF%BC%89/3.png" class><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拆分数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 尝试使用逻辑回归算法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">history = lr.fit(X_train, y_train) <span class="comment"># 训练机器</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;逻辑回归预测准确率&#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(lr.score(X_test, y_test)*<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 逻辑回归预测准确率78.35%</span></span><br></pre></td></tr></table></figure><p>上面的结果是用上节的逻辑回归做的。下面我们尝试使用神经网络算法，准确率会不会提高。</p><h2 id="单隐层网络的keras实现">单隐层网络的keras实现</h2><h3 id="1-用序贯模型构建网络">1.用序贯模型构建网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential <span class="comment"># 导入序贯模型</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense <span class="comment"># 导入全连接层</span></span><br></pre></td></tr></table></figure><ul><li>序贯模型，也可以叫做顺序模型，是最常用的深度网络层和层间的架构，也就是一个层接着一个层顺序堆叠。</li><li>密集（dense）层，是最常用的深度网路层的类型，也称全连接层，即当前层和下一层的所有神经元都连接。</li></ul><p>搭建网络模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ann = Sequential()</span><br><span class="line">ann.add(Dense(units=<span class="number">12</span>, input_dim=<span class="number">12</span>, activation=<span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加输入层</span></span><br><span class="line">ann.add(Dense(units=<span class="number">24</span>, activation=<span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加隐层</span></span><br><span class="line">ann.add(Dense(units=<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)) <span class="comment"># 添加输出层</span></span><br><span class="line">ann.summary() <span class="comment"># 显示网络模型（此语句非必要，只用作显示） </span></span><br></pre></td></tr></table></figure><img src="/2022/04/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%C2%B7%E7%BB%AD%E4%B8%80%EF%BC%89/4.png" class><blockquote><p>summary方法显示了神经网络的结构，包括每个层的类型、输出张量的形状、参数数量以及整个网络的参数数量。上面这个网络有3层，493个参数（就是每个神经元的权重等），这对于神经网络来说，参数数量已经是比较少了。</p></blockquote><p>通过下面的代码还可以展示出神经网络的形状结构：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> SVG <span class="comment"># 实现神经网络结构的图形化显示</span></span><br><span class="line"><span class="keyword">from</span> keras.utils.vis_utils impo</span><br><span class="line">rt model_to_dot</span><br><span class="line">SVG(model_to_dot(ann, show_shapes=<span class="literal">True</span>).create(prog=<span class="string">&#x27;dot&#x27;</span>,<span class="built_in">format</span>=<span class="string">&#x27;svg&#x27;</span>))</span><br></pre></td></tr></table></figure><blockquote><p>模型的创建：ann=Sequential()创建了一个序贯神经网络模型。在Keras中，绝大部分的神经网络都是通过序贯模型所创建的。另外还有一种模型，称为函数式API，可以创建更为复杂的网络结构。<br>输入层：通过add方法，可开始神经网络层的堆叠，序贯模型，也就是一层一层的顺序堆叠。</p><blockquote><p>Dense是层的类型，代表密集层网络，是神经网络层中最基本的层，也叫全连接层。类似的还有CNN中的Conv2D层，RNN中的LSTM层，等等。<br>input_dim是输入维度，输入维度必须维度必须和特征维度相同。<br>unit是输出维度，设置为12，这个值是随意选择的，这代表了经过线性变化和激活之后的假设空间维度，也就是神经元的个数。维度越大，则模型的覆盖面也越大，但是模型就越复杂，需要的计算量就越多。<br>activation是激活函数，这是每一层都需要设置的参数。这里的激活函数选择的是&quot;relu&quot;，而不是Sigmoid.<br>隐层：仍然通过add方法。在输入层之后的所有层都不需要重新指定输入维度，因为网络能够通过上一层的输出自动地调整。这一层的类型同样是全连接层。<br>输出层：仍然是一个全连接层，指定的输出维度是1。因为对于二分类问题，输出维度必须是1.<strong>对于二分类问题的输出层，Sigmoid是固定的选择。</strong> 如果是<strong>用神经网络解决回归问题的话，那么输出层不用指定任何激活函数。</strong></p></blockquote></blockquote><p>下面编译刚才建好的这个网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编译神经网络，指定优化器、损失函数，以及评估指标</span></span><br><span class="line">ann.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, <span class="comment"># 优化器</span></span><br><span class="line">loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, <span class="comment"># 损失函数</span></span><br><span class="line">metric=[<span class="string">&#x27;acc&#x27;</span>]) <span class="comment"># 评估指标</span></span><br></pre></td></tr></table></figure><p>用Sequential模型的complie方法对整个网络进行编译时，需要指定以下几个关键参数。</p><blockquote><p>优化器（optimizer）：一般情况下，“adam”或者“rmsprop”都是很好的优化器选项，但也有其他可选的优化器。<br>损失函数（loss）：对于二分类问题来说，基本上二元交叉熵函数（bc）是固定选项；如果是用神经网络解决线性的回归问题，那么均方误差函数是合适的选择。<br>评估指标（metrics）：这里采用预测准确率acc（也就是accuracy的缩写，两者在代码中的是等价）作为评估网络性能的标准；而对于回归问题，平均误差函数是合适的选择。准确率，也就是正确地预测占全部数据的比重，是最为常用的分类评估指标。</p></blockquote><h3 id="2-全连接层">2.全连接层</h3><p>全连接层（Dense层）是最常见的神经网络层，用于处理最普通的机器学习向量数据集，即形状为（标签，样本）的2D张量数据集，其实现了一个逻辑回归功能：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo>=</mo><mi>A</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>d</mi><mi>o</mi><mi>t</mi><mo stretchy="false">(</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo separator="true">,</mo><mi>k</mi><mi>e</mi><mi>r</mi><mi>n</mi><mi>e</mi><mi>l</mi><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><mi>i</mi><mi>a</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Output = Activation(dot(input,kernel)+bias) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mord mathnormal">u</span><span class="mord mathnormal">tp</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">c</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">d</span><span class="mord mathnormal">o</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal">in</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.02778em;">er</span><span class="mord mathnormal">n</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">bia</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span></span></p><p>公式中的kernel，其实就是权重。网络是多节点的，所以它从向量升级为矩阵，把输入和权重矩阵做点积，然后加上一个属于该层的偏置，激活之后，就得到了全连接层往下一层的输出。偏置是可有可无的，不是必需项。<br>其实，每层最基本的、必须设置的参数只有以下两个。</p><ul><li>units:输出维度。</li><li>activation:激活函数。</li></ul><p>对于输入层，当然还要多指定一个输入维度。对于后面的隐层和输出层，则连输入维度也可以省略。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dense(unit=<span class="number">12</span>,</span><br><span class="line">                activation=<span class="literal">None</span>,</span><br><span class="line">                use_bias=<span class="literal">True</span>,</span><br><span class="line">                kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">                bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">                kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">                bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">                activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">                kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">                bias_constraint=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h3 id="3-其他层">3.其他层</h3><p>常见的其他层还有：</p><ul><li>循环层（如Keras的LSTM层），用于处理保存在形状为（样本，时戳，标签）的3D张量中的序列数据。</li><li>二维卷积层（如Keras的Conv2D层），用于处理保存在形状为（样本，帧数，图像高度，图像宽度，颜色深度）的4D张量中的图像数据。</li></ul><h2 id="训练单隐层神经网络">训练单隐层神经网络</h2><p>通过fit方法实现拟合过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">history=ann.fit(X_train, y_train,</span><br><span class="line">            epochs=<span class="number">30</span>,</span><br><span class="line">            batch_size=<span class="number">64</span>,</span><br><span class="line">            validation_data=(X_test, y_test))</span><br></pre></td></tr></table></figure><ul><li>batch_size：用于指定数据批量，也就是每一次梯度下降更新参数时所同时训练的样本数量。这是利用了CPU/GPU的并行计算功能，系统默认值是32。</li><li>validation_data：用于指定验证集。这样就可以一边用训练集网络，一边验证某评估网络的效果。为了简化模型，就直接使用测试集来验证。</li></ul><img src="/2022/04/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%C2%B7%E7%BB%AD%E4%B8%80%EF%BC%89/5.png" class><p>我这里最后的准确率是79.15%，还算不错，虽然我不太清楚这个数据是否具有可信度。</p><h2 id="训练过程的图形化显示">训练过程的图形化显示</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_history</span>(<span class="params">history</span>):</span><br><span class="line">  loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">  val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line">  epochs = <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(loss)+<span class="number">1</span>)</span><br><span class="line">  plt.figure(figsize=(<span class="number">12</span>, <span class="number">4</span>))</span><br><span class="line">  plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">  plt.plot(epochs, loss, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;Training loss&#x27;</span>)</span><br><span class="line">  plt.plot(epochs, val_loss, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Validatiion loss&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Training and validation loss&#x27;</span>)</span><br><span class="line">  plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">  plt.legend()</span><br><span class="line">  acc = history.history[<span class="string">&#x27;acc&#x27;</span>]</span><br><span class="line">  val_acc = history.history[<span class="string">&#x27;val_acc&#x27;</span>]</span><br><span class="line">  plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">  plt.plot(epochs, acc, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;Training acc&#x27;</span>)</span><br><span class="line">  plt.plot(epochs, val_acc, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Validation acc&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Training and validation accuracy&#x27;</span>)</span><br><span class="line">  plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">  plt.legend()</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line">show_history(history)</span><br></pre></td></tr></table></figure><img src="/2022/04/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%C2%B7%E7%BB%AD%E4%B8%80%EF%BC%89/6.png" class title="训练集和验证集上的损失曲线和准确率曲线"><h1>分类数据不平衡问题</h1><p>看上去79.15%的准确率挺高的，还蛮不戳的，但实际上，这一次的预测结果是非常失败的。因为原本数据集中，10000个客户里面有大约8000个客户都是不会离开的，那我预测所有用户都不会离开，不也猜对了80%嘛，大差不差。那这情况是什么引起的呢？是因为<strong>数据集中标签类别数据非常不平衡，一方太多一少。</strong> 下面给出一个极端的例子来讲解。</p><h2 id="例子">例子</h2><p>假设有一个手机厂商，每天生产1000台手机，有天1000台手机中出现了2台次品，现在需要通过机器学习来预测一个模型，可以展示该厂商的次品率。最后得到结果如下表：</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;用keras单隐层网络预测客户流失率&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;这一节的实例过程中出现的结果，每个人之间可能都有较大出入。大家主要还是要理解代码的思路！另外下载教学资源中，此节的数据集和kaggle上作者上传的数据有所差别，强烈建议下载kaggle上的数据集：&lt;a </summary>
      
    
    
    
    <category term="人工智能学习路线" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    <category term="从零开始机器学习" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="人工智能" scheme="https://yigexiaogai.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="机器学习，毕设相关" scheme="https://yigexiaogai.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E6%AF%95%E8%AE%BE%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战（五）</title>
    <link href="https://yigexiaogai.github.io/2022/04/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%EF%BC%89/"/>
    <id>https://yigexiaogai.github.io/2022/04/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%EF%BC%89/</id>
    <published>2022-04-23T08:19:38.000Z</published>
    <updated>2022-06-04T14:01:41.247Z</updated>
    
    <content type="html"><![CDATA[<h1>深度神经网络——找出可能流失的客户</h1><p>这次的实战，是有关于一个金融领域的项目，而它需要神经网络，也就是深度学习模型来解决。深度学习是一个进阶领域，涉及到的新内容较多，因此此节内容会比较长，我会分成几个博客来写。<br>该项目的具体需求是根据已知的一批客户数据，来预测某个银行的客户是否会流失。<strong>通过学习历史数据，如果机器能够判断出哪些客户很有可能在未来两年内结束在该银行的业务，那么银行的工作人员就可以采用相应的措施来挽留这些高流失风险的客户。</strong><br>其实这个问题和上一节的心脏病预测问题一样，本质上都是<strong>分类</strong>，而我们需要研究的就是，神经网络解决这一问题有什么优势。<br>本节用到的数据集在第5课源码包“教学用例 银行客户流失”目录中的“Bank Customer.csv”。</p><h1>神经网络原理</h1><h2 id="传统机器算法的局限性">传统机器算法的局限性</h2><p>传统机器算法拥有一定局限性。首先，越简单的关系越容易拟合。然而对于一个非线性的问题，我们就需要通过复杂的函数模型（如高阶多项式）去拟合。此时单纯的线性回归函数已经不能满足要求，因而我们需要把特征重新组合，变化出新的特征。这种对特征的变换、升阶，以及多个特征相互组合形成新特征的过程，就是机器学习过程中即耗时又耗力的<strong>特征工程</strong>的一个例子。<strong>当特征的维度越来越大时，特征之间相互组合的可能性将以几何级数递增，特征空间急剧膨胀，对应的假设空间也随之膨胀，此时简单的模型已经不够看了。</strong><br>计算机对于**非结构化数据（感知类数据）**不够敏感，例如图片、音乐等，因此对于它们的处理，计算机就不能使用简单的线性回归或者逻辑回归了。（你想，计算机看一张长宽为50像素的图片，就有2500个特征了哦！）<br>而神经网络就是专门为了解决这里超高特征维度的感知类问题诞生的。</p><h2 id="神经网络的优势">神经网络的优势</h2><p>神经网络实际上就像一张规模巨大的铁道网路，一头是上车的乘客，一头是乘客要到的站点，根据车票（特征），我们要让乘客正确到站（标签），这之间要经过重重的过渡站点，每次都要判定一下乘客的类别。如果第一次送错了，我们就要检查网路中的权重，让犯错的权重受到惩罚，而给判断正确的权重给予奖励。<br>书中用了一个分类猫图片还是狗图片的例子，相当生动，建议阅读书中原文。<br>因此精炼语言总结一下神经网络的机理：<strong>它是用一串一串的函数，也就是层，堆叠起来，作用于输入数据，进行从原始数据到分类结果的过滤于提纯。这些层通过权重来参数化，通过损失函数来判断当前网络的效能，然后通过优化器来调整权重，寻找从输入到输出的最佳函数。</strong> 注意以下两点：</p><ul><li>学习：就是为神经网络的每个层中的每个神经元寻找最佳的权重。</li><li>知识：就是学到的权重。</li></ul><h1>从感知器到单隐层网络</h1><p>神经网络由神经元组成，最简单的神经网络只有一个神经元，叫<strong>感知器</strong>。</p><h2 id="感知器是最基本的神经元">感知器是最基本的神经元</h2><p>下图代表一个神经元，它可以接受输入，并根据输入提供一个输出。</p><img src="/2022/04/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%EF%BC%89/1.jpg" class><p>我们把图简单调整一下，左边的特征为(x0=b=1, x1, x2)，权重为(w0=-30, w1=20, w2=20)，激活函数为sigmoid，此时这个神经元变成了什么呢？它是一个<strong>与门</strong>！<br>看表：</p><table><thead><tr><th>x1</th><th>x2</th><th>z(x)</th><th>g(z(x))</th><th>逻辑值</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>-30</td><td>0.00001</td><td>0</td></tr><tr><td>0</td><td>1</td><td>-10</td><td>0.00001</td><td>0</td></tr><tr><td>1</td><td>0</td><td>-10</td><td>0.00001</td><td>0</td></tr><tr><td>1</td><td>1</td><td>40</td><td>0.99999</td><td>1</td></tr></tbody></table><p>这就是最简单的一个逻辑回归的拟合，拟合函数是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>20</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>20</mn><msub><mi>x</mi><mn>2</mn></msub><mo>−</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">z(x)=20x_1+20x_2-30</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7944em;vertical-align:-0.15em;"></span><span class="mord">20</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7944em;vertical-align:-0.15em;"></span><span class="mord">20</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">30</span></span></span></span></p><p>它还能变成一个<strong>或门</strong>哦！只要把权重变成(w0=-10, w1=20, w2=20)<br>看表：</p><table><thead><tr><th>x1</th><th>x2</th><th>z(x)</th><th>g(z(x))</th><th>逻辑值</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>-10</td><td>0.00001</td><td>0</td></tr><tr><td>0</td><td>1</td><td>10</td><td>0.99999</td><td>1</td></tr><tr><td>1</td><td>0</td><td>10</td><td>0.99999</td><td>1</td></tr><tr><td>1</td><td>1</td><td>20</td><td>0.99999</td><td>1</td></tr><tr><td>拟合函数变成了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>20</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>20</mn><msub><mi>x</mi><mn>2</mn></msub><mo>−</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">z(x)=20x_1+20x_2-10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7944em;vertical-align:-0.15em;"></span><span class="mord">20</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7944em;vertical-align:-0.15em;"></span><span class="mord">20</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">10</span></span></span></span></td><td></td><td></td><td></td><td></td></tr></tbody></table><h2 id="假设空间要覆盖特征空间">假设空间要覆盖特征空间</h2><p>单神经元，通过训练可以用作逻辑回归分类器，那么它是如何进化成更复杂的多层神经网络呢？<br>我们需要重温几个概念。</p><ul><li>输入空间： x，输入值的集合。</li><li>输出空间： y，输出值的集合。通常，输出空间会小于输入空间</li><li>特征空间：每一个样本被称作一个实例，通常由特征向量表示，所有特征向量存在的空间称为特征空间。特征空间有时候于输入空间相同，有时候不同。有时候经过特征工程之后，输入空间可通过某种映射生成新的特征空间。</li><li>假设空间： 假设空间一般是对于学习到的模型（即函数）而言的。模型表达了输入到输出的一种映射集合。假设空间代表着<strong>模型学习过程中能够覆盖的最大范围</strong></li></ul><h2 id="单神经元特征空间的局限性">单神经元特征空间的局限性</h2><p>上节提到的感知器可以成功拟合“与门”和“或门”，但是却拟合不了“同或门”或者“异或门”。<br>比如举一个“同或门”的例子：</p><table><thead><tr><th>x1</th><th>x2</th><th>逻辑值</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>1</td></tr><tr><td>0</td><td>1</td><td>0</td></tr><tr><td>1</td><td>0</td><td>0</td></tr><tr><td>1</td><td>1</td><td>1</td></tr></tbody></table><p><strong>无论你怎么调整函数权重，都无法做到这个事情</strong>。其实我们可以用初中的<strong>线性规划</strong>知识证明这一点！<br>想一想，在一个平面直角坐标系中，你要画一条线，让点（1，1）和（0，0）在直线上方，并且（1，0）和（0，1）在直线下方，你会发现，你画不出这一条线。我们来代数推算一下：<br>直线方程：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>b</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">w_1x_1+w_2x_2+b=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span><br>目标：找到合适的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">w_1,w_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><br>满足：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>w</mi><mn>1</mn><mo>+</mo><mi>w</mi><mn>2</mn><mo>+</mo><mi>b</mi><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>b</mi><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>w</mi><mn>1</mn><mo>+</mo><mi>b</mi><mo>&lt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>w</mi><mn>2</mn><mo>+</mo><mi>b</mi><mo>&lt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\begin{cases}        w1+w2+b&gt;0 \\        b&gt;0\\        w1+b&lt;0 \\        w2+b&lt;0 \\      \end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:5.76em;vertical-align:-2.63em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.95em;"><span style="top:-1.6em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎩</span></span></span><span style="top:-1.592em;"><span class="pstrut" style="height:3.15em;"></span><span style="height:0.916em;width:0.8889em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.8889em" height="0.916em" style="width:0.8889em" viewbox="0 0 888.89 916" preserveaspectratio="xMinYMin"><path d="M384 0 H504 V916 H384z M384 0 H504 V916 H384z"/></svg></span></span><span style="top:-3.15em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎨</span></span></span><span style="top:-4.292em;"><span class="pstrut" style="height:3.15em;"></span><span style="height:0.916em;width:0.8889em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.8889em" height="0.916em" style="width:0.8889em" viewbox="0 0 888.89 916" preserveaspectratio="xMinYMin"><path d="M384 0 H504 V916 H384z M384 0 H504 V916 H384z"/></svg></span></span><span style="top:-5.2em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎧</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.45em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.13em;"><span style="top:-5.13em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">0</span></span></span><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">0</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">0</span></span></span><span style="top:-0.81em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.63em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><br>将上两式相加，和下两式相加，你会发现矛盾：<br><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>w</mi><mn>1</mn><mo>+</mo><mi>w</mi><mn>2</mn><mo>+</mo><mn>2</mn><mi>b</mi><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>w</mi><mn>1</mn><mo>+</mo><mi>w</mi><mn>2</mn><mo>+</mo><mn>2</mn><mi>b</mi><mo>&lt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\begin{cases}    w1+w2+2b&gt;0 \\    w1+w2+2b&lt;0 \\ \end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3em;vertical-align:-1.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">2</span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">0</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">2</span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p><p><strong>因此，我们需要增加网络的层数解决这个问题</strong>，其实，再增加一层网络，就相当于我们在线性规划的时候，多了一条可以使用的直线。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;深度神经网络——找出可能流失的客户&lt;/h1&gt;
&lt;p&gt;这次的实战，是有关于一个金融领域的项目，而它需要神经网络，也就是深度学习模型来解决。深度学习是一个进阶领域，涉及到的新内容较多，因此此节内容会比较长，我会分成几个博客来写。&lt;br&gt;
该项目的具体需求是根据已知的一批客户数</summary>
      
    
    
    
    <category term="人工智能学习路线" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    <category term="从零开始机器学习" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="人工智能" scheme="https://yigexiaogai.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="机器学习，毕设相关" scheme="https://yigexiaogai.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E6%AF%95%E8%AE%BE%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战（四·续）</title>
    <link href="https://yigexiaogai.github.io/2022/04/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%9B%9B%C2%B7%E7%BB%AD%EF%BC%89/"/>
    <id>https://yigexiaogai.github.io/2022/04/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%9B%9B%C2%B7%E7%BB%AD%EF%BC%89/</id>
    <published>2022-04-21T08:36:04.000Z</published>
    <updated>2022-06-04T14:01:51.713Z</updated>
    
    <content type="html"><![CDATA[<h1>问题定义：确定鸢尾花的种类（多元分类）</h1><p>与第三节一样，我将多元的机器学习放在续节中。<br>确定鸢尾花的种类是一个经典的多分类问题。数据来自R.A. Fisher1936年发表的论文。数据集中的鸢尾花共3类，分别为山鸢尾（iris-setosa）、杂色鸢尾（iris-versicolor）、维吉尼亚鸢尾（iris-virginica）。<br>数据集中共有150个数据，已经按照标签类别排序，每类50个数据，其中有一类可以和其他两类进行线性的分割，但另外两类无法根据特征线性分割开。<br>特征和标签字段如下所示：<br>————————————————</p><ul><li>Id: 序号</li><li>SepalLengthCm: 花萼长度</li><li>SepalWidthCm: 花萼宽度</li><li>PetalLengthCm: 花瓣长度</li><li>PetalWidthCm: 花瓣宽度</li><li>Species: 种类（标签）</li></ul><p>————————————————</p><h2 id="从二元分类到多元分类">从二元分类到多元分类</h2><p>其实从二元分类到多元分类的思想我觉得很好理解，其实就是逐个击破。每一次分类都是一次二元分类，一类接着一类分出来，而不是一下就把所有类别分好。最后一类出来的时候就是分类完成了。</p><h2 id="多元分类的损失函数">多元分类的损失函数</h2><p>多元分类的标签有以下两种格式：</p><ul><li>一种是one-hot格式的分类编码，比如，数字0~9分类中的数字8，格式为[0,0,0,0,0,0,0,1,0]</li><li>一种是直接转换为类别数字，如1、2、3、4</li></ul><p>因此损失函数也有以下两种情况：</p><ul><li>如果通过one-hot分类编码输出标签，则应使用分类交叉熵（categorical crossentropy）作为损失函数</li><li>如果输出的标签编码为类别数字，则应使用稀疏分类交叉熵（sparse categorical crossentropy）</li></ul><h1>正则化</h1><p>正则化技术是为了解决过拟合问题，这在上一节的模型中也出现了过拟合现象。如果不了解<a href="https://baike.baidu.com/item/%E6%AC%A0%E6%8B%9F%E5%90%88/22692155?fr=aladdin">欠拟合</a>和<a href="https://baike.baidu.com/item/%E8%BF%87%E6%8B%9F%E5%90%88/3359778?fr=aladdin">过拟合</a>现象，建议看一下书，或者点击百度。</p><p>简单来说原理其实相当易懂，我们在机器学习的时候，其实是希望模型越简单越好的，谁都不想用一个100次方的函数当模型吧！虽然，如果你有100个点的话，100次方的函数一定能全给你拟合咯，但是这样的模型在用新数据的时候就会表现的很差。但是我们在面对数据的时候，咋能知道有没有过拟合呢？确实，这其实是一个无法绝对避免的问题，我们只能尽可能缓解，所以，<strong>我们的目的就是，用尽可能简单的模型做到优秀的拟合效果</strong>。所以我们可以在<strong>损失函数</strong>中加入一个<strong>惩罚项</strong>,如果模型很复杂，那这一项就变大，这样损失值就大了，强调了这次训练的“不好”;反之，模型简单就减小这项的值，表示我奖励你这个模型，在复杂度上还蛮不错哦！<br>正则化具体实现，我们需要引入一个模型参数λ，看一个具体例子：<br>加入了正则化参数之后的线性回归均方误差损失函数公式：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mi>M</mi><mi>S</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munder><mo>∑</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>D</mi></mrow></munder><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mfrac><mi>λ</mi><mrow><mn>2</mn><mi>N</mi></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>w</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">L(w,b)=MSE=\cfrac{1}{N}\sum_{(x,y)\in D}(y-h(x))^2+\cfrac{\lambda}{2N}\sum_{i=1}^n w_i^2 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">MSE</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.516em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.809em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">λ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.453em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span></p><ul><li>后面那一项就是正则化项，至于λ的值，可能需要手动调整。（我觉得这也是一个<strong>超参数</strong>吧）</li></ul><h1>通过逻辑回归解决多元分类问题</h1><h2 id="数据分析和准备">数据分析和准备</h2><p>Sklearn自带这个数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris() <span class="comment"># 导入iris</span></span><br><span class="line">X_sepal = iris.data[:, [<span class="number">0</span>, <span class="number">1</span>]] <span class="comment"># 花萼特征集：两个特征集长度和宽度</span></span><br><span class="line">X_petal = iris.data[:, [<span class="number">2</span>, <span class="number">3</span>]] <span class="comment"># 花瓣特征集：两个特征集长度和宽度</span></span><br><span class="line"></span><br><span class="line">y = iris.target</span><br></pre></td></tr></table></figure><p>我们可以把三种鸢尾花的分类可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(iris.data[:, [<span class="number">0</span>]][iris.target==<span class="number">0</span>], iris.data[:, [<span class="number">1</span>]][iris.target==<span class="number">0</span>], <span class="string">&#x27;ro&#x27;</span>)</span><br><span class="line">plt.plot(iris.data[:, [<span class="number">0</span>]][iris.target==<span class="number">1</span>], iris.data[:, [<span class="number">1</span>]][iris.target==<span class="number">1</span>], <span class="string">&#x27;bx&#x27;</span>)</span><br><span class="line">plt.plot(iris.data[:, [<span class="number">0</span>]][iris.target==<span class="number">2</span>], iris.data[:, [<span class="number">1</span>]][iris.target==<span class="number">2</span>], <span class="string">&#x27;g^&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&quot;iris-setosa&quot;</span>, <span class="string">&quot;iris-versicolor&quot;</span>, <span class="string">&quot;iris-virginica&quot;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&quot;Sepal length&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Sepal width&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Sepal Length VS Width&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/2022/04/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%9B%9B%C2%B7%E7%BB%AD%EF%BC%89/1.png" class><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler <span class="comment"># 导入标准化工具库</span></span><br><span class="line">X_train_sepal, X_test_sepal, y_train_sepal, y_test_sepal = train_test_split(X_sepal, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;花瓣训练集样本数：&quot;</span>, <span class="built_in">len</span>(X_train_sepal))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;花瓣测试集样本数：&quot;</span>, <span class="built_in">len</span>(X_test_sepal))</span><br><span class="line">scalar = StandardScaler()</span><br><span class="line">X_train_sepal = scaler.fit_transform(X_train_sepal)</span><br><span class="line">X_test_sepal = scaler.transform(X_test_sepal)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并特征集和标签集，留待以后展现数据</span></span><br><span class="line">X_combined_sepal = np.vstack((X_train_sepal, X_test_sepal))</span><br><span class="line">y_combined_sepal np.hstack((y_train_sepal, y_test_sepal))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 花瓣训练集样本数： 105</span></span><br><span class="line"><span class="comment"># 花瓣测试集样本数： 45</span></span><br></pre></td></tr></table></figure><h2 id="通过sklearn实现逻辑回归的多元分类">通过sklearn实现逻辑回归的多元分类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">lr = LogisticRegression(penalty=<span class="string">&#x27;l2&#x27;</span>, C=<span class="number">0.1</span>) <span class="comment"># 设定L2正则化和C参数</span></span><br><span class="line">lr.fit(X_train_sepal, y_train_sepal)</span><br><span class="line">score = lr.score(X_test_sepal, y_test_sepal)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;SKlearn逻辑回归测试准确率&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(score*<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># SKlearn逻辑回归测试准确率68.89</span></span><br></pre></td></tr></table></figure><ul><li>L2正则化，只是选择了正则化的参数类别，但是用多大的力度进行呢？此时要引入另外一个配套用到正则化相关参数C。C表示正则化的力度，它与λ刚好成反比。C值越小，正则化力度越大。</li></ul><h2 id="正则化参数——C值的选择">正则化参数——C值的选择</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_decision_regions</span>(<span class="params">X, y, classifier, test_idx=<span class="literal">None</span>, resolution=<span class="number">0.02</span></span>):</span><br><span class="line">    markers = (<span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;v&#x27;</span>)</span><br><span class="line">    colors = (<span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;blues&#x27;</span>, <span class="string">&#x27;lightgreen&#x27;</span>)</span><br><span class="line">    color_Map = ListedColormap(color[:<span class="built_in">len</span>(np.unique(y))])</span><br><span class="line">    x1_min = X[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span></span><br><span class="line">    x1_max = X[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    x2_min = X[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span></span><br><span class="line">    x2_max = X[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))</span><br><span class="line">    Z = classifier.predict(np.arange([xx1.ravel(), xx2.ravel()]).T)</span><br><span class="line">    Z = Z.reshape(xx1.shape)</span><br><span class="line">    plt.contour(xx1, xx2, Z, alpha=<span class="number">0.4</span>, cmap = color_Map)</span><br><span class="line">    plt.xlim(xx1.<span class="built_in">min</span>(), xx1.<span class="built_in">max</span>())</span><br><span class="line">    plt.ylim(xx2.<span class="built_in">min</span>(), xx2.<span class="built_in">max</span>())</span><br><span class="line">    X_test, Y_test = X[test_idx, :], y[test_idx]</span><br><span class="line">    <span class="keyword">for</span> idx, cl <span class="keyword">in</span> <span class="built_in">enumerate</span>(np.unique(y)):</span><br><span class="line">        plt.scatter(x=X[y==cl, <span class="number">0</span>], y=X[y==cl, <span class="number">1</span>], alpha=<span class="number">0.8</span>, c=color_Map(idx), marker=markers[idx], label=cl)</span><br></pre></td></tr></table></figure><p>然后使用不同的C值进行逻辑回归分类，并绘制分类结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">C_param_range = [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>]</span><br><span class="line">sepal_acc_tabel = pd.DataFrame(columns=<span class="string">&#x27;C_parameter&#x27;</span>, <span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">sepal_acc_tabel[<span class="string">&#x27;C_parameter&#x27;</span>] = C_param_range</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">j = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> C_param_range:</span><br><span class="line">    lr = LogisticRegression(penalty=<span class="string">&#x27;L2&#x27;</span>, C=i, random_state=<span class="number">0</span>)</span><br><span class="line">    lr.fit(X_train_sepal, y_train_sepal)</span><br><span class="line">    y_pred_sepal = lr.predict(X_test_sepal)</span><br><span class="line">    sepal_acc_table.iloc[j,<span class="number">1</span>] = accuracy_score(y_test_sepal, y_pred_sepal)</span><br><span class="line">    j += <span class="number">1</span></span><br><span class="line">    plt.subplot(<span class="number">3</span>, <span class="number">2</span>, j)</span><br><span class="line">    plt.subplots_abjust(hspace=<span class="number">0.4</span>)</span><br><span class="line">    plot_desision_regions(X=X_combined_sepal, y=y_combined_sepal, classifier=lr,test_idx=<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">150</span>))</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Sepal length&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Sepal width&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;C=%s&#x27;</span>%i)</span><br></pre></td></tr></table></figure><img src="/2022/04/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%9B%9B%C2%B7%E7%BB%AD%EF%BC%89/2.png" class><p>如果C值为10重做逻辑回归：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lr = LogisticRegression(penalty=<span class="string">&#x27;l2&#x27;</span>, C=<span class="number">10</span>)</span><br><span class="line">lr.fit(X_train_sepal, y_train_sepal) <span class="comment"># 训练机器</span></span><br><span class="line">score = lr.score(X_test_sepal, y_test_sepal) <span class="comment"># 测试集分数评估</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;SKlearn逻辑回归测试准确率&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(score*<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># SKlearn逻辑回归测试准确率80.00</span></span><br></pre></td></tr></table></figure><h1>课后练习</h1><blockquote><p>一 根据这一节的练习案例数据集，泰坦尼克数据集（见源码包），并使用本节的方法完成逻辑回归。</p></blockquote><blockquote><p>二 在多元分类中，我们基于鸢尾花特征，进行了多元分类，请用类似的方法进行花瓣特征集的分类。</p></blockquote><blockquote><p>三 基于花瓣特征集，进行正则化参数C值的调试。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;问题定义：确定鸢尾花的种类（多元分类）&lt;/h1&gt;
&lt;p&gt;与第三节一样，我将多元的机器学习放在续节中。&lt;br&gt;
确定鸢尾花的种类是一个经典的多分类问题。数据来自R.A. Fisher1936年发表的论文。数据集中的鸢尾花共3类，分别为山鸢尾（iris-setosa）、杂色鸢</summary>
      
    
    
    
    <category term="人工智能学习路线" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    <category term="从零开始机器学习" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="人工智能" scheme="https://yigexiaogai.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="机器学习，毕设相关" scheme="https://yigexiaogai.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E6%AF%95%E8%AE%BE%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战（四）</title>
    <link href="https://yigexiaogai.github.io/2022/04/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
    <id>https://yigexiaogai.github.io/2022/04/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%9B%9B%EF%BC%89/</id>
    <published>2022-04-17T07:43:53.000Z</published>
    <updated>2022-06-04T14:01:48.468Z</updated>
    
    <content type="html"><![CDATA[<h1>逻辑回归——给病患和鸢尾花分类</h1><h1>问题定义：判断客户是否患病</h1><p>机器学习主要有两大应用，一是回归问题，二是分类问题。在上节已经结束了回归算法后，这节开始我们要了解<strong>逻辑回归</strong>(logistic regression)。为什么逻辑回归也有“回归”两个字呢，因为本质上来说，逻辑回归算法也是通过调整权重w和偏置b来找到找到线性函数来计算数据样本属于某一类的概率。<br>现在有这样一个实际问题：我们现在因实际需要，想要知道一个人的心脏健康状况，例如<strong>一个人有心脏病的可能性</strong>。在研究这个问题之前，我们首先需要查看大量医院心脏病例的大量资料，查看可能导致心脏病的因素，比如先天因素，后天因素等。现在呢假设我们已经有个1000个人的资料了，然后我们从统计和经验出发，归纳出几个可能会影响心脏病的因素：<br>——————————</p><ul><li>age: 年龄</li><li>sex: 性别</li><li>cp: 胸痛类型</li><li>trestbps: 休息时血压</li><li>chol: 胆固醇</li><li>fbs: 血糖</li><li>restecg: 心电图</li><li>thalach: 最大心率</li><li>exang: 运动后心绞痛</li><li>oldpeak: 运动后ST段压低</li><li>slope: 运动高峰期ST段的斜率</li><li>ca: 主动脉荧光造影染色数</li><li>thal: 缺陷种类</li><li>target: 0代表有无心脏病，1代表有心脏病<br>——————————</li></ul><blockquote><p>也就是说，target是我们要调查的目标，也就是标签。其他的特征。<br>在收集到的数据中，要注意，有的人target是1，也就是有心脏病，有的人是0，但还有些人是<strong>空白</strong>，他们可能自己也不知道有没有心脏病。<strong>这些数据就是无标签的数据</strong>。</p></blockquote><h1>从回归问题到分类问题</h1><p>其实从回归问题到分类问题，可以直观感受到，问题的解决领域是从连续到离散的。<strong>回归问题要求我们拟合一个连续的函数，而分类问题要求我们输出一个明确的类别</strong>。在输出明确的离散分类值之前，算法首先输出的其实是一个<strong>可能性，可以把这个可能性理解成一个概率。</strong></p><ul><li>机器学习模型根据输入数据判断一个人患心脏病的可能性为80%，那么就把这个人判定为“患病”类，输出1。</li><li>机器学习模型根据输入数据判断一个人患心脏病的可能性为30%，那么就把这个人判定为“健康”类，输出0。</li></ul><h2 id="用线性回归-阶跃函数完成分类">用线性回归+阶跃函数完成分类</h2><p>这一节图例较多，强烈建议大家去看一下书中的原内容，讲的很简单易懂！<br>我们这里拿一个例子打比方：考试。<br>**考试没到60分则输出0，表示不及格；60分以上则输出1，表示及格。**因此统计图大概如下：</p><img src="/2022/04/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%9B%9B%EF%BC%89/1.png" class><p>这种图你要画一条什么函数去拟合呢？显然不可能是一次函数吧！二次函数好像也不行……总而言之，我们找到了下面这样一条函数：</p><img src="/2022/04/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%9B%9B%EF%BC%89/2.png" class><p>它的名字叫<strong>sigmoid函数</strong>，是阶跃函数的一种。书中还讲到了一种拟合方法，是将回归函数到转换成分段函数进行拟合，但是比较容易受误差过大的数据的影响，这里就不介绍了。<br>sigmoid函数的公式如下：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">f(x)=\cfrac{1}{1+e^{-x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3593em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6973em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span></span></span></span></span></span></span></p><h2 id="逻辑函数的假设函数">逻辑函数的假设函数</h2><p>有了sigmoid函数，就可以开始正式建立逻辑回归的机器学习模型。<strong>重点要确定假设函数h(x)，来预测y’。</strong><br>总结以下之前的内容，把线性回归和逻辑函数整合起来，形成逻辑回归的假设函数。<br>（1）首先通过线性回归模型求出一个中间值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo separator="true">,</mo><mi>z</mi><mo>=</mo><msub><mi>w</mi><mn>0</mn></msub><msub><mi>x</mi><mn>0</mn></msub><mo>+</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo>+</mo><msub><mi>w</mi><mi>n</mi></msub><msub><mi>x</mi><mi>n</mi></msub><mo>=</mo><msup><mi>W</mi><mi>T</mi></msup><mi>X</mi></mrow><annotation encoding="application/x-tex">z,z=w_0x_0+w_1x_1+...+w_nx_n=W^TX</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">...</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span>。它是一个连续值，区间并不在[0, 1]之间，可能小于0或者大于1，范围从无穷小到无穷大。<br>（2）然后通过逻辑函数把这个中间值z转化成0~1的概率值，以提高拟合效果<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">f(x)=\cfrac{1}{1+e^{-x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3593em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6973em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span></span></span></span></span></span><br>（3）结合步骤（1）和（2），把新的函数表示为假设函数的形式：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mo stretchy="false">(</mo><msup><mi>W</mi><mi>T</mi></msup><mi>X</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">h(x)=\cfrac{1}{1+e^{-(W^TX)}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3942em;vertical-align:-0.8042em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.2791em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8309em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7741em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8042em;"><span></span></span></span></span></span><span></span></span></span></span></span></span></p><p>这个值也就是逻辑回归算法得到的y’。<br>（4）最后还要根据y’所代表的概率，确定分类结果。</p><blockquote><p>如果h(x)&gt;=0.5，分类结果为1<br>如果h(X)&lt;0.5，分类结果为0</p></blockquote><p>上述过程的关键在于选择Sigmoid函数进行从线性回归到逻辑回归的转换。Sigmoid函数的优点如下：</p><ul><li>Sigmoid函数是连续函数，具有单调递增性（类似于递增的线性函数）。</li><li>Sigmoid函数具有可微性，可以进行微分，也可以进行求导。</li><li>输出范围为[0, 1]，结果可以表示为概率的形式，为分类做出准备。</li><li>抑制分类的两边，对中间区域的细微变化敏感，这对分类结果拟合效果好。</li></ul><h2 id="逻辑回归的损失函数">逻辑回归的损失函数</h2><p>把训练集中所有的预测所得概率和实际结果的差异求和，并取平均值，就可以得到平均误差，这就是逻辑回归的损失函数：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munder><mo>∑</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>D</mi></mrow></munder><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(w,b)=\cfrac{1}{N}\sum_{(x,y) \in D}Loss(h(x),y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.516em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.809em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">oss</span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span></p><p>在逻辑回归中，我们不能使用线性回归中的损失函数MSE（均方误差函数），因为在逻辑回归中，MSE对于w和b而言不再是一个凸函数，无法使用梯度下降。<br>为了避免陷入局部最低点，我们为逻辑回归选择了符合条件的新的损失函数，公式如下：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>y</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>y</mi><mo>=</mo><mn>0</mn><mo separator="true">,</mo><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex"> \begin{cases}    y=1, Loss(h(x),y)=-log(h(x))\\    y=0, Loss(h(x),y)=-log(1-h(x))\end{cases} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3em;vertical-align:-1.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">oss</span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">oss</span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>整合得到损失函数如下：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mo>−</mo><mn>1</mn></mrow><mi>N</mi></mfrac><munder><mo>∑</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>D</mi></mrow></munder><mo stretchy="false">[</mo><mi>y</mi><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">L(w,b)=\cfrac{-1}{N}\sum_{(x,y)\in D}[ylog(h(x))+(1-y)log(1-h(x))] </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.516em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.809em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516em;"><span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))]</span></span></span></span></span></p><h2 id="逻辑回归的梯度下降">逻辑回归的梯度下降</h2><p>逻辑回归的梯度下降和线性回归一样，也是先进行微分，然后把计算出来的导数乘以学习速率α，通过不断的迭代，更新w和b，直至收敛。<br>梯度计算公式如下：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>梯度</mtext><mo>=</mo><msup><mi>h</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi mathvariant="normal">∂</mi><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac><mi>L</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi mathvariant="normal">∂</mi><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac><mo stretchy="false">{</mo><mfrac><mrow><mo>−</mo><mn>1</mn></mrow><mi>N</mi></mfrac><munder><mo>∑</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>D</mi></mrow></munder><mo stretchy="false">[</mo><mi>y</mi><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex"> 梯度=h&#x27;(x)=\cfrac{\partial}{\partial w}L(w,b)=\cfrac{\partial}{\partial w}\{\cfrac{-1}{N}\sum_{(x,y)\in D}[ylog(h(x))+(1-y)log(1-h(x))]\} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">梯度</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0519em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.276em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.516em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mopen">{</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.809em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516em;"><span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))]}</span></span></span></span></span></p><p>计算得到最终结果：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>梯度</mtext><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mo stretchy="false">(</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><mo stretchy="false">(</mo><mi>w</mi><mo>⋅</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>⋅</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">梯度=\cfrac{1}{N}\sum_{i=1}^N(y^{(i)}-(w \cdot x^{(i)}))\cdot x^{(i)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">梯度</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.938em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p>引入学习速率之后，参数随梯度变化而更新的公式如下：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>w</mi><mo>=</mo><mi>w</mi><mo>−</mo><mi>α</mi><mo>⋅</mo><mfrac><mi mathvariant="normal">∂</mi><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac><mi>L</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w=w-\alpha \cdot \cfrac{\partial}{\partial w}L(w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4445em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.276em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span></span></p><p>即</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>w</mi><mo>=</mo><mi>w</mi><mo>−</mo><mfrac><mi>α</mi><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mo stretchy="false">(</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><mo stretchy="false">(</mo><mi>w</mi><mo>⋅</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>⋅</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">w=w-\cfrac{\alpha}{N}\sum_{i=1}^N(y^{(i)}-(w \cdot x^{(i)}))\cdot x^{(i)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.938em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span></p><h1>通过逻辑回归解决二元分类问题</h1><h2 id="数据的准备与分析">数据的准备与分析</h2><p>数据的准备其实在一开始就已经比较完善了。<br>——————————</p><ul><li>age: 年龄</li><li>sex: 性别</li><li>cp: 胸痛类型</li><li>trestbps: 休息时血压</li><li>chol: 胆固醇</li><li>fbs: 血糖</li><li>restecg: 心电图</li><li>thalach: 最大心率</li><li>exang: 运动后心绞痛</li><li>oldpeak: 运动后ST段压低</li><li>slope: 运动高峰期ST段的斜率</li><li>ca: 主动脉荧光造影染色数</li><li>thal: 缺陷种类</li><li>target: 0代表有无心脏病，1代表有心脏病<br>——————————<br>使用以下代码读取数据：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df_heart=pd.read_csv(<span class="string">&quot;/content/drive/MyDrive/Colab Notebooks/heart.csv&quot;</span>)</span><br><span class="line">df_heart.head()</span><br></pre></td></tr></table></figure><p>用value_counts方法输出数据集中患心脏病和没有患心脏病的人数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df_heart.target.value_counts() <span class="comment"># 输出分类值，以及各类别数目</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1    165</span></span><br><span class="line"><span class="comment"># 0    138</span></span><br><span class="line"><span class="comment"># Name: target, dtype: int64</span></span><br></pre></td></tr></table></figure><blockquote><p>这个步骤是很有必要的。因为如果某一类别比例特别低（例如300个数据中只有3个人患病），那么这样的数据可能不适合用逻辑回归的方法作分类。</p></blockquote><p>可以对某些数据进行相关性的分析，例如可以显示年龄、最大心率这两个特征与是否患病之间的关系：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 以年龄+最大心率作为输入，查看分类结果散点图</span></span><br><span class="line">plt.scatter(x=df_heart.age[df_heart.target==<span class="number">1</span>],</span><br><span class="line">            y=df_heart.thalach[df_heart.target==<span class="number">1</span>],</span><br><span class="line">            c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.scatter(x=df_heart.age[df_heart.target==<span class="number">0</span>],</span><br><span class="line">            y=df_heart.thalach[df_heart.target==<span class="number">0</span>],</span><br><span class="line">            marker=<span class="string">&#x27;^&#x27;</span>) <span class="comment"># 以三角形表示</span></span><br><span class="line">plt.legend([<span class="string">&#x27;Disease&#x27;</span>, <span class="string">&#x27;No disease&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Age&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Heart Rate&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>输出结果如下图：</p><img src="/2022/04/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%9B%9B%EF%BC%89/3.png" class><ul><li>输出结果表示心率越高，患心脏病的可能性就越大。</li></ul><p>下面构建特征集和标签集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = df_heart.drop([<span class="string">&#x27;target&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">y = df_heart.target.values</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量X的形状：&quot;</span>, X.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量y的形状：&quot;</span>, y.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 张量X的形状： (303, 13)</span></span><br><span class="line"><span class="comment"># 张量y的形状： (303, 1)</span></span><br></pre></td></tr></table></figure><p>拆分数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><p><strong>数据特征缩放：</strong> 我们使用sklearn中内置的函数缩放器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScalar <span class="comment"># 导入数据缩放器</span></span><br><span class="line">scalar = MinMaxScalar()</span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br><span class="line">X_test = scalar.transform(X_test)</span><br></pre></td></tr></table></figure><ul><li>这里有个点大家会发现，我们对训练集和测试集调用缩放器方法不同，一个是fit_transform（先拟合再应用），一个是transform（直接应用）。<strong>这是因为，所有的最大值、最小值、均值、标准差等数据缩放的中间值，都要从训练集得来，然后同样的值应用到训练集和测试集。</strong> 标签集已经不需要归一化了，本来就只有0&amp;1。</li></ul><h2 id="建立逻辑回归模型">建立逻辑回归模型</h2><p>数据准备工作结束后，下面构建逻辑回归模型。<br><strong>1.逻辑函数的定义</strong><br>首先定义Sigmoid函数，一会儿会调用它：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先定义一个Sigmoid函数，输入Z，返回y&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    y_hat=<span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> y_hat</span><br></pre></td></tr></table></figure><p><strong>2.损失函数的定义</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_function</span>(<span class="params">X,y,w,b</span>):</span><br><span class="line">    y_hat = sigmoid(np.dot(X,w)+b) <span class="comment"># Sigmoid逻辑函数+线性函数（wx+b）得到y&#x27;</span></span><br><span class="line">    loss=-((y*np,log(y_hat)+(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-y_hat)))</span><br><span class="line">    cost = np.<span class="built_in">sum</span>(loss)/X.shape[<span class="number">0</span>] <span class="comment"># 整个数据集的平均损失</span></span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><ul><li>在这里我们保留了b，不把它当作w0看待，所以不需要对数据集加一列1。这里的线性回归函数是多变量的，因此（X，w）点积操作之后，Sigmoid函数进行逻辑转换生成y’。<br>y’生成过程中需要注意的仍然是点积操作中张量X和W的形状。</li><li>X——(242，13), 2D矩阵。</li><li>W——(13, 1), 也是2D矩阵，因为第二阶为1，也可以看作向量</li><li>点积之后生成的y_hat，就是一个形状为(242,1)的张量，其中存储了每一个样本的预测值。</li></ul><p><strong>3.梯度下降的实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">X, y, w, b, lr, <span class="built_in">iter</span></span>):</span><br><span class="line">    l_history = np.zeros(<span class="built_in">iter</span>)</span><br><span class="line">    w_history = np.zeros((<span class="built_in">iter</span>, w.shape[<span class="number">0</span>], w.shape[<span class="number">1</span>]))</span><br><span class="line">    b_history = np.zeros(<span class="built_in">iter</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">iter</span>):</span><br><span class="line">        y_hat = sigmoid(np.dot(X, w)+b)</span><br><span class="line">        loss = -(y*np.log(y_hat)+(<span class="number">1</span>-y_hat)*np.log(<span class="number">1</span>-y_hat))</span><br><span class="line">        derivative_w = np.dot(X.T, ((y_hat-y))/X.shape[<span class="number">0</span>])</span><br><span class="line">        derivative_d = np.<span class="built_in">sum</span>(loss)/X.shape[<span class="number">0</span>]</span><br><span class="line">        w = w - lr*derivative_w</span><br><span class="line">        d = d - lr*derivative_b</span><br><span class="line">        l_history[i] = loss_function(X, y, w, b)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;轮次&quot;</span>, i+<span class="number">1</span>, <span class="string">&quot;当前轮次训练集损失：&quot;</span>, l_history[i])</span><br><span class="line">        w_history[i] = w</span><br><span class="line">        b_history[i] = b</span><br><span class="line">    <span class="keyword">return</span> l_history, w_history, b_history</span><br></pre></td></tr></table></figure><ul><li>这里要注意w_history是一个3D张量，因为w已经是一个2D张量了。</li></ul><p><strong>4.分类预测的实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    z=np.dot(X, w) + b</span><br><span class="line">    y_hat=sigmoid(z)</span><br><span class="line">    y_pred=np.zeros((y_hat.shape[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(y_hat.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">if</span> y_hat[i, <span class="number">0</span>]&lt;<span class="number">0.5</span>:</span><br><span class="line">            y_pred[i, <span class="number">0</span>]=<span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_pred[i, <span class="number">0</span>]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure><h2 id="开始训练机器">开始训练机器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">logistic_regression</span>(<span class="params">X, y, w, b, lr, <span class="built_in">iter</span></span>):</span><br><span class="line">    l_history, w_history, b_history = gradient_descent(X, y, w, b, lr, <span class="built_in">iter</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;训练最终损失：&quot;</span>, l_history[-<span class="number">1</span>])</span><br><span class="line">    y_pred = predict(X, w_history[-<span class="number">1</span>], b_history[-<span class="number">1</span>])</span><br><span class="line">    training_acc = <span class="number">100</span> - np.mean(np.<span class="built_in">abs</span>(y_pred-y_train))*<span class="number">100</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;逻辑回归训练准确率：&#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(training_acc)) <span class="comment"># 输出准确率</span></span><br><span class="line">    <span class="keyword">return</span> l_history, w_history, b_history</span><br></pre></td></tr></table></figure><p>初始化参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dimension = X.shape[<span class="number">1</span>]</span><br><span class="line">weight = np.full((dimension, <span class="number">1</span>), <span class="number">0.1</span>)</span><br><span class="line">bias=<span class="number">0</span></span><br><span class="line"><span class="comment"># 初始化超参数</span></span><br><span class="line">alpha = <span class="number">1</span></span><br><span class="line">iterations = <span class="number">500</span></span><br></pre></td></tr></table></figure><p>调用回归模型，训练机器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_history, weight_history, bias_history = logistic_regression(X_train, y_train, weight, bias, alpha, iterations)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">轮次 1 当前轮次训练集损失： 0.6151245497498954</span><br><span class="line">轮次 2 当前轮次训练集损失： 0.5579913862647583</span><br><span class="line">...</span><br><span class="line">轮次 499 当前轮次训练集损失： 0.2932648260062251</span><br><span class="line">轮次 500 当前轮次训练集损失： 0.29324449197008057</span><br><span class="line">训练最终损失： 0.29324449197008057</span><br><span class="line">逻辑回归训练准确率：87.19</span><br></pre></td></tr></table></figure><h2 id="测试分类结果">测试分类结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y_pred = predict(X_test, weight_history[-<span class="number">1</span>], bias_history[-<span class="number">1</span>]) <span class="comment"># 预测测试集</span></span><br><span class="line">testing_acc = <span class="number">100</span> - np.mean(np.<span class="built_in">abs</span>(y_pred-y_test))*<span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;逻辑回归测试准确率：&#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(testing_acc))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 逻辑回归测试准确率：83.61</span></span><br><span class="line"><span class="comment"># 可以查看分类预测的具体值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;逻辑回归预测分类值：&quot;</span>, predict(X_test, weight_history[-<span class="number">1</span>], bias_history[-<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><h2 id="绘制损失曲线">绘制损失曲线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">loss_history_test  = np.zeros(iterations) <span class="comment"># 初始化历史损失</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">    loss_history_test[i]=loss_function(X_test, y_test, weight_historu[i], bias_history[i])</span><br><span class="line">index = np.arange(<span class="number">0</span>, iterations, <span class="number">1</span>)</span><br><span class="line">plt.plot(index, loss_history, c=<span class="string">&#x27;blue&#x27;</span>, linestyle=<span class="string">&#x27;solid&#x27;</span>)</span><br><span class="line">plt.plot(index, loss_history_test, c=<span class="string">&#x27;red&#x27;</span>, linestyle=<span class="string">&#x27;dashed&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&quot;Training data&quot;</span>, <span class="string">&quot;Test Loss&quot;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&quot;Number of Iteration&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Cost&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/2022/04/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%9B%9B%EF%BC%89/4.png" class><ul><li>从图中看出，有时候损失曲线会存在上升趋势的现象，是明显的过拟合现象，因此我们要调整一下超参数。</li></ul><h2 id="直接调用sklearn库">直接调用sklearn库</h2><p>直接用库的函数也可以达到同样的效果，就是我们上面这么多的工作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;SK learn逻辑回归测试准确率&#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(lr.score(X_test, y_test)*<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># SK learn逻辑回归测试准确率81.97%</span></span><br></pre></td></tr></table></figure><h2 id="哑特征的使用">哑特征的使用</h2><p>在特征中间，有一些字段的值表示类别，比如“cp”字段，取值为1、2、3、4，这些表示的都是<strong>胸痛类型，本身不存在大小关系</strong>，但是计算机可能会觉得，4&gt;1，意义就发生了改变。所以我们需要使用哑特征来解决这一“误解”的现象。<br>在构建特征值和字段值前，写如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把三个文本变量转换为哑变量</span></span><br><span class="line">a = pd.get_dummies(df_heart[<span class="string">&#x27;cp&#x27;</span>], prefix=<span class="string">&#x27;cp&#x27;</span>)</span><br><span class="line">b = pd.get_dummies(df_heart[<span class="string">&#x27;thal&#x27;</span>], prefix=<span class="string">&#x27;thal&#x27;</span>)</span><br><span class="line">c = pd.get_dummies(df_heart[<span class="string">&#x27;slope&#x27;</span>], prefix=<span class="string">&#x27;slope&#x27;</span>)</span><br><span class="line"><span class="comment"># 把哑变量添加进dataframe</span></span><br><span class="line">frames= [df_heart, a, b, c]</span><br><span class="line">df_heart = pd.concat(frames, axis=<span class="number">1</span>)</span><br><span class="line">df_heart = df_heart.drop(columns=[<span class="string">&#x27;cp&#x27;</span>,<span class="string">&#x27;thal&#x27;</span>,<span class="string">&#x27;slope&#x27;</span>])</span><br><span class="line">df_heart.head()</span><br></pre></td></tr></table></figure><img src="/2022/04/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%9B%9B%EF%BC%89/5.png" class>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;逻辑回归——给病患和鸢尾花分类&lt;/h1&gt;
&lt;h1&gt;问题定义：判断客户是否患病&lt;/h1&gt;
&lt;p&gt;机器学习主要有两大应用，一是回归问题，二是分类问题。在上节已经结束了回归算法后，这节开始我们要了解&lt;strong&gt;逻辑回归&lt;/strong&gt;(logistic regressio</summary>
      
    
    
    
    <category term="人工智能学习路线" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    <category term="从零开始机器学习" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="人工智能" scheme="https://yigexiaogai.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="机器学习，毕设相关" scheme="https://yigexiaogai.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E6%AF%95%E8%AE%BE%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战（三·续）</title>
    <link href="https://yigexiaogai.github.io/2022/04/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%C2%B7%E7%BB%AD%EF%BC%89/"/>
    <id>https://yigexiaogai.github.io/2022/04/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%C2%B7%E7%BB%AD%EF%BC%89/</id>
    <published>2022-04-14T09:51:43.000Z</published>
    <updated>2022-06-04T14:01:58.665Z</updated>
    
    <content type="html"><![CDATA[<h1>用轮廓图描绘L、w和b的关系</h1><p>在上一节，我们已经基本完成了机器学习的建模过程。接下来要介绍的一个辅助性工作叫做轮廓图。损失曲线描绘的是损失和迭代次数之间的关系，而轮廓图则描绘的是L、w和b这三者之间的关系。<br><strong>下面是补充代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上面是上一节的代码</span></span><br><span class="line"><span class="comment"># 使用contour plot</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.animation <span class="keyword">as</span> animation</span><br><span class="line">theta0_vals = np.linspace(-<span class="number">2</span>, <span class="number">3</span>, <span class="number">100</span>)</span><br><span class="line">theta1_vals = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>)</span><br><span class="line">J_vals = np.zeros((theta0_vals.size, theta1_vals.size))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t1, element <span class="keyword">in</span> <span class="built_in">enumerate</span>(theta0_vals):</span><br><span class="line">  <span class="keyword">for</span> t2, element2 <span class="keyword">in</span> <span class="built_in">enumerate</span>(theta1_vals):</span><br><span class="line">    thetaT = np.zeros(shape=(<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">    weight = element</span><br><span class="line">    bias = element2</span><br><span class="line">    J_vals[t1, t2] = loss_function(X_train, y_train, weight, bias)</span><br><span class="line"></span><br><span class="line">J_vals = J_vals.T</span><br><span class="line">A, B = np.meshgrid(theta0_vals, theta1_vals)</span><br><span class="line">C = J_vals</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">plt.plot(X_train, y_train, <span class="string">&#x27;ro&#x27;</span>, label=<span class="string">&#x27;Training data&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Sales Prediction&#x27;</span>)</span><br><span class="line">plt.axis([X_train.<span class="built_in">min</span>()-X_train.std(), X_train.<span class="built_in">max</span>()+X_train.std(),</span><br><span class="line">          y_train.<span class="built_in">min</span>()-y_train.std(), y_train.<span class="built_in">max</span>()+y_train.std()])</span><br><span class="line">plt.grid(axis=<span class="string">&#x27;both&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Wechat Ads Volumn(X1)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Sales Volumn(Y)&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;lower right&#x27;</span>)</span><br><span class="line"></span><br><span class="line">line, = plt.plot([], [], <span class="string">&#x27;b-&#x27;</span>, label=<span class="string">&#x27;Current Hypothesis&#x27;</span>)</span><br><span class="line">annotation = plt.text(-<span class="number">2</span>, <span class="number">3</span>, <span class="string">&#x27;&#x27;</span>, fontsize=<span class="number">20</span>, color=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">annotation.set_animated(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">cp = plt.contour(A, B, C)</span><br><span class="line">plt.colorbar(cp)</span><br><span class="line">plt.title(<span class="string">&#x27;Filled Contours Plot&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Bias&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Weight&#x27;</span>)</span><br><span class="line">track, = plt.plot([],[],<span class="string">&#x27;r-&#x27;</span>)</span><br><span class="line">point, = plt.plot([],[],<span class="string">&#x27;ro&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init</span>():</span><br><span class="line">  line.set_data([],[])</span><br><span class="line">  track.set_data([],[])</span><br><span class="line">  point.set_data([],[])</span><br><span class="line">  annotation.set_text(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">  <span class="keyword">return</span> line, track, point, annotation</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">animate</span>(<span class="params">i</span>):</span><br><span class="line">  fit1_X = np.linspace(X_train.<span class="built_in">min</span>()-X_train.std(), </span><br><span class="line">              X_train.<span class="built_in">max</span>()+X_train.std(), <span class="number">1000</span>)</span><br><span class="line">  fit2_y = bias_history[i]+weight_history[i] * fit1_X</span><br><span class="line"></span><br><span class="line">  fit2_X = bias_history.T[:i]</span><br><span class="line">  fit2_y = weight_history.T[:i]</span><br><span class="line"></span><br><span class="line">  track.set_data(fit2_X, fit2_y)</span><br><span class="line">  line.set_data(fit1_X, fit1_y)</span><br><span class="line">  point.set_data(bias_history.T[i], weight_history.T[i])</span><br><span class="line"></span><br><span class="line">  annotation.set_text(<span class="string">&#x27;Cost = %.4f&#x27;</span> %(loss_history[i]))</span><br><span class="line">  <span class="keyword">return</span> line, track, point, annotation</span><br><span class="line"></span><br><span class="line">anim = animation.FuncAnimation(fig, animate, init_func=init,</span><br><span class="line">                  frames=<span class="number">50</span>, interval=<span class="number">0</span>, blit=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">anim.save(<span class="string">&#x27;animation.gif&#x27;</span>, writer=<span class="string">&#x27;imagemagick&#x27;</span>, fps = <span class="number">500</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示Contour Plot动画</span></span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> HTML</span><br><span class="line"></span><br><span class="line">filename = <span class="string">&#x27;animation.gif&#x27;</span></span><br><span class="line"></span><br><span class="line">video = io.<span class="built_in">open</span>(filename, <span class="string">&#x27;r+b&#x27;</span>).read()</span><br><span class="line">encoded = base64.b64encode(video)</span><br><span class="line">HTML(data=<span class="string">&#x27;&#x27;&#x27;&lt;img src=&quot;data:image/gif;base64,&#123;0&#125;&quot; type=&quot;gif&quot; /&gt;&#x27;&#x27;&#x27;</span>.<span class="built_in">format</span>(encoded.decode(<span class="string">&#x27;ascii&#x27;</span>)))</span><br></pre></td></tr></table></figure><ul><li><strong>其中anim.save(‘animation.gif’, writer=‘imagemagick’, fps = 500)在colab会报错，需要安装imagemagick这个依赖项</strong>，但是colab好像需要购买colab pro才能安装别的依赖……总之我尝试之后不太行。</li></ul><p>这是源代码中展示的最终效果图：</p><img src="/2022/04/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%C2%B7%E7%BB%AD%EF%BC%89/anime.gif" class><h1>实现多元线性回归模型</h1><p>多元，也就是说特征是多维的。我们使用下标代表特征的编号，采用以下多元（多变量）的线性方程式来构造假设函数。</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>y</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>b</mi><mo>+</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><msub><mi>w</mi><mn>3</mn></msub><msub><mi>x</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">y&#x27;=h(x)=b+w_1x_1+w_2x_2+w_3x_3 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9963em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p><hr><p>在机器学习的程序设计中，这个公式可以被向量化地实现。</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>y</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>w</mi><mi>T</mi></msup><mo>⋅</mo><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y&#x27;=h(x)=w^T \cdot x+b </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9963em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></span></p><p><strong>还可以进一步把公式简化</strong>，也就是把b看作权重<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">w_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，则需要引入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，公式变成：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>y</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>w</mi><mn>0</mn></msub><msub><mi>x</mi><mn>0</mn></msub><mo>+</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>3</mn></msub><msub><mi>x</mi><mn>3</mn></msub><mo>+</mo><msub><mi>w</mi><mn>4</mn></msub><msub><mi>x</mi><mn>4</mn></msub></mrow><annotation encoding="application/x-tex">y&#x27;=h(x)=w_0x_0+w_1x_1+w_3x_3+w_4x_4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9963em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p><p>且：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><msub><mi>x</mi><mn>0</mn></msub><mo>=</mo><mi>b</mi><mo>×</mo><mn>1</mn><mo>=</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">w_0x_0=b \times 1=b </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></span></p><p>简写为：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>w</mi><mi>T</mi></msup><mo>⋅</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">h(x)=w^T \cdot x </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span></p><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line"><span class="comment"># df表示dataframe，一种数据结构</span></span><br><span class="line">df_ads = pd.read_csv(<span class="string">&#x27;/content/drive/MyDrive/Colab Notebooks/advertising.csv&#x27;</span>)</span><br><span class="line">df_ads.head() <span class="comment"># .head()方法就是读取数据集前5行，便于预览</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns <span class="comment"># 为统计学数据可视化工具库</span></span><br><span class="line"><span class="comment"># 对所有的标签和特征两两显示相关性的热力图</span></span><br><span class="line">sns.heatmap(df_ads.corr(), cmap=<span class="string">&#x27;YlGnBu&#x27;</span>, annot=<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">sns.pairplot(df_ads,</span><br><span class="line">      x_vars=[<span class="string">&#x27;wechat&#x27;</span>, <span class="string">&#x27;weibo&#x27;</span>, <span class="string">&#x27;others&#x27;</span>],</span><br><span class="line">      y_vars=<span class="string">&#x27;sales&#x27;</span>,</span><br><span class="line">      height=<span class="number">4</span>, aspect=<span class="number">1</span>, kind=<span class="string">&#x27;scatter&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X = np.array(df_ads) <span class="comment"># 构建特征集</span></span><br><span class="line">X = np.delete(X, [<span class="number">3</span>], axis=<span class="number">1</span>)</span><br><span class="line">y = np.array(df_ads.sales) <span class="comment"># 构建标签集</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量X的阶：&quot;</span>, X.ndim)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的形状：&quot;</span>, X.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的内容：\n&quot;</span>, X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># y = y.reshape(len(y), 1)</span></span><br><span class="line">y = y.reshape(-<span class="number">1</span>, <span class="number">1</span>) <span class="comment"># 等同于上个语句</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;张量y的形状:&quot;</span>, y.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 张量y的形状: (200, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集进行80%（训练集）和20%（测试集）的分割</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scalar</span>(<span class="params">train, test</span>): <span class="comment"># 定义归一化函数，进行数据压缩</span></span><br><span class="line">  <span class="built_in">min</span> = train.<span class="built_in">min</span>(axis=<span class="number">0</span>)</span><br><span class="line">  <span class="built_in">max</span> = train.<span class="built_in">max</span>(axis=<span class="number">0</span>)</span><br><span class="line">  gap = <span class="built_in">max</span> - <span class="built_in">min</span> <span class="comment"># 最大值和最小值的差</span></span><br><span class="line">  train -= <span class="built_in">min</span> <span class="comment"># 所有数值减去最小值</span></span><br><span class="line">  train /= gap <span class="comment"># 所有数据除以最大值和最小值的差</span></span><br><span class="line">  test -= <span class="built_in">min</span> <span class="comment"># 把训练集最小值应用于测试集</span></span><br><span class="line">  test /= gap <span class="comment"># 把训练集最大值和最小值的差应用于测试集</span></span><br><span class="line">  <span class="keyword">return</span> train, test</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">min_max_gap</span>(<span class="params">train</span>):</span><br><span class="line">  <span class="built_in">min</span>=train.<span class="built_in">min</span>(axis=<span class="number">0</span>)</span><br><span class="line">  <span class="built_in">max</span>=train.<span class="built_in">max</span>(axis=<span class="number">0</span>)</span><br><span class="line">  gap = <span class="built_in">max</span> - <span class="built_in">min</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">min</span>, <span class="built_in">max</span>, gap</span><br><span class="line"></span><br><span class="line">y_min, y_max, y_gap = min_max_gap(y_train)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">X_train_original = X_train.copy() <span class="comment"># 保留一份训练集数据副本，用于对要预测数据归一化</span></span><br><span class="line"></span><br><span class="line">X_train, X_test = scalar(X_train, X_test)</span><br><span class="line">y_train, y_test = scalar(y_train, y_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">f, (ax1, ax2, ax3) = plt.subplots(<span class="number">1</span>,<span class="number">3</span>,sharey=<span class="literal">True</span>,figsize=(<span class="number">20</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">ax1.plot(X_train[:,<span class="number">0</span>:<span class="number">1</span>], y_train, <span class="string">&#x27;ro&#x27;</span>, label=<span class="string">&#x27;WeChat Training data&#x27;</span>)</span><br><span class="line">ax1.set_title(<span class="string">&#x27;WeChat&#x27;</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line"></span><br><span class="line">ax2.plot(X_train[:,<span class="number">1</span>:<span class="number">2</span>], y_train, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;Weibo Training data&#x27;</span>)</span><br><span class="line">ax2.set_title(<span class="string">&#x27;Weibo&#x27;</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line"></span><br><span class="line">ax3.plot(X_train[:,<span class="number">2</span>:<span class="number">3</span>], y_train, <span class="string">&#x27;go&#x27;</span>, label=<span class="string">&#x27;Others Training data&#x27;</span>)</span><br><span class="line">ax3.set_title(<span class="string">&#x27;Others&#x27;</span>)</span><br><span class="line">ax3.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ul><li>这里我们将三个维度的散点图可视化</li></ul><img src="/2022/04/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%C2%B7%E7%BB%AD%EF%BC%89/1.png" class><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x0_train = np.ones((<span class="built_in">len</span>(X_train), <span class="number">1</span>)) <span class="comment"># 构造X_train长度的全1数组配合对偏置的点积</span></span><br><span class="line">X_train = np.append(x0_train, X_train, axis=<span class="number">1</span>) <span class="comment"># 把X增加一系列的1</span></span><br><span class="line">x0_test = np.ones((<span class="built_in">len</span>(X_test), <span class="number">1</span>)) <span class="comment"># 构造X_test长度的全1数组配合对偏置的点积</span></span><br><span class="line">X_test = np.append(x0_test, X_test, axis=<span class="number">1</span>) <span class="comment"># 把X增加一系列的1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量X的形状&quot;</span>, X_train.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 张量X的形状 (160, 4)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_function</span>(<span class="params">X, y, W</span>): <span class="comment"># 手动定义一个均方误差函数</span></span><br><span class="line">  y_hat = X.dot(W.T)</span><br><span class="line">  loss = y_hat.reshape((<span class="built_in">len</span>(y_hat),<span class="number">1</span>)) - y <span class="comment"># 每一个y&#x27;和训练集中真实的y之间的差异</span></span><br><span class="line">  cost = np.<span class="built_in">sum</span>(loss ** <span class="number">2</span>) / (<span class="number">2</span>*<span class="built_in">len</span>(X)) <span class="comment"># 这是均方误差函数的代码实现</span></span><br><span class="line">  <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 梯度下降</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">X, y, W, lr, iterations</span>):</span><br><span class="line">  l_history = np.zeros(iterations) <span class="comment"># 初始化记录梯度下降过程损失的数组</span></span><br><span class="line">  W_history = np.zeros((iterations, <span class="built_in">len</span>(W))) <span class="comment"># 初始化记录梯度下降过程权重的数组</span></span><br><span class="line">  <span class="keyword">for</span> <span class="built_in">iter</span> <span class="keyword">in</span> <span class="built_in">range</span>(iterations): <span class="comment"># 进行梯度下降的迭代，就是下多少级台阶</span></span><br><span class="line">    y_hat = X.dot(W.T)</span><br><span class="line">    loss = y_hat.reshape((<span class="built_in">len</span>(y_hat), <span class="number">1</span>)) - y</span><br><span class="line">    derivative_W = X.T.dot(loss)/<span class="built_in">len</span>(X)</span><br><span class="line">    derivative_W = derivative_W.reshape(<span class="built_in">len</span>(W))</span><br><span class="line">    W = W - lr * derivative_W</span><br><span class="line">    l_history[<span class="built_in">iter</span>] = loss_function(X, y, W)</span><br><span class="line">    W_history[<span class="built_in">iter</span>] = W</span><br><span class="line">  <span class="keyword">return</span> l_history, W_history <span class="comment"># 返回梯度下降过程中的数据</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确定参数的初始值</span></span><br><span class="line">iteration = <span class="number">300</span> <span class="comment"># 迭代300次</span></span><br><span class="line">alpha = <span class="number">0.25</span> <span class="comment"># 初始学习速率为1</span></span><br><span class="line">weight = np.array([<span class="number">0.5</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]) <span class="comment"># 权重</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;当前损失：&quot;</span>, loss_function(X_train, y_train, weight))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当前损失： 0.8039183733604857</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义线性回归模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear_regression</span>(<span class="params">X, y, weight, alpha, iteration</span>):</span><br><span class="line">  loss_history, weight_history = gradient_descent(X, y, weight, alpha, iteration)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;训练最终损失：&quot;</span>, loss_history[-<span class="number">1</span>]) <span class="comment"># 打印最终损失</span></span><br><span class="line">  y_pred = X.dot(weight_history[-<span class="number">1</span>])</span><br><span class="line">  training_acc = <span class="number">100</span> - np.mean(np.<span class="built_in">abs</span>(y_pred-y)*<span class="number">100</span>) <span class="comment"># 计算准确率</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;线性回归训练准确率：&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(training_acc))</span><br><span class="line">  <span class="keyword">return</span> loss_history, weight_history</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用刚才定义的线性回归模型</span></span><br><span class="line">loss_history, weight_history = linear_regression(X_train, y_train, weight, alpha, iteration) <span class="comment">#训练机器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出各个权重随梯度下降的变化图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">4</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">plt.plot(loss_history, <span class="string">&#x27;g--&#x27;</span>, label=<span class="string">&#x27;Loss Curve&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Iteration&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">plt.plot(weight_history, <span class="string">&#x27;b--&#x27;</span>, label=<span class="string">&#x27;Weight Curve&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Iteration&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Weight&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(weight_history[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练最终损失： 0.0020534948072892627</span></span><br><span class="line"><span class="comment"># 线性回归训练准确率：75.99</span></span><br><span class="line"><span class="comment"># [0.05592132 0.64217276 0.22460039 0.0889083 ]</span></span><br></pre></td></tr></table></figure><p>曲线变化如图：</p><img src="/2022/04/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%C2%B7%E7%BB%AD%EF%BC%89/2.png" class><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X_plan = [<span class="number">250</span>, <span class="number">50</span>, <span class="number">50</span>]</span><br><span class="line">X_train, X_plan = scalar(X_train_original, X_plan)</span><br><span class="line">X_plan = np.append([<span class="number">1</span>], X_plan)</span><br><span class="line">y_plan = np.dot(weight_history[-<span class="number">1</span>], X_plan)</span><br><span class="line">y_value = y_plan*<span class="number">23.8</span>+<span class="number">3.2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预计商品销售额：&quot;</span>, y_value, <span class="string">&quot;千元&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预计商品销售额： 7.944991957387483 千元</span></span><br></pre></td></tr></table></figure><h1>课后练习</h1><blockquote><p>一 请用sklearn库的线性回归函数完成同样的任务。</p></blockquote><blockquote><p>二 在sklearn库中，除了前面介绍过的线性回归(Linear Regression)算法以外，还有岭回归(Ridge Regression)和套索回归(Lasso Regression)这两种变体。请尝试使用这两种线性回归算法，并应用于本课的数据集</p></blockquote><blockquote><p>导入第3课的练习数据集：Keras自带的波士顿房价数据集，并使用本课介绍的方法完成线性回归，实现对标签的预测。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;用轮廓图描绘L、w和b的关系&lt;/h1&gt;
&lt;p&gt;在上一节，我们已经基本完成了机器学习的建模过程。接下来要介绍的一个辅助性工作叫做轮廓图。损失曲线描绘的是损失和迭代次数之间的关系，而轮廓图则描绘的是L、w和b这三者之间的关系。&lt;br&gt;
&lt;strong&gt;下面是补充代码：&lt;/st</summary>
      
    
    
    
    <category term="人工智能学习路线" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    <category term="从零开始机器学习" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="人工智能" scheme="https://yigexiaogai.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="机器学习，毕设相关" scheme="https://yigexiaogai.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E6%AF%95%E8%AE%BE%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战（三）</title>
    <link href="https://yigexiaogai.github.io/2022/04/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <id>https://yigexiaogai.github.io/2022/04/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/</id>
    <published>2022-04-08T14:43:20.000Z</published>
    <updated>2022-06-04T14:01:55.004Z</updated>
    
    <content type="html"><![CDATA[<h1>线性回归——预测网店的销售额</h1><h1>问题定义</h1><p>张三在运营一个网店，他发现网店商品的销量和广告推广的力度息息相关。张三准备好了以下几个问题亟待解决：</p><ul><li>（1）各种广告和商品销售额的相关度如何？</li><li>（2）各种广告和商品销售额之间体现出一种什么关系？</li><li>（3）哪一种广告对于商品销售额影响最大？</li><li>（4）分配特定的广告投放金额，预测出未来的商品销售额。</li></ul><h1>数据的收集和预处理</h1><h2 id="收集网店销售额数据">收集网店销售额数据</h2><p>已经收集好的销售额的数据，在数据集中主要包含以下内容：<br>特征：1.微信投放广告开支 2.微博投放广告开支 3.其他平台投放广告开支<br>标签：商品销售额</p><h2 id="数据读取和可视化">数据读取和可视化</h2><p>首先进行<strong>数据可视化</strong>的工作，先要对这份数据有个直观的了解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">df_ads = pd.read_csv(/content/drive/MyDrive/Colab Notebooks/advertising.csv)</span><br><span class="line">df_ads.head()</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wechatweibootherssales</span><br><span class="line">0304.493.6294.49.7</span><br><span class="line">11011.934.4398.416.7</span><br><span class="line">21091.132.8295.217.3</span><br><span class="line">385.5173.6403.27.0</span><br><span class="line">41047.0302.4553.622.1</span><br></pre></td></tr></table></figure><ul><li>使用colab时，我们将数据集文件上传到和.ipynb文件同目录下，但是不能使用pd.read_csv(‘advertising.csv’)，我们可以在笔记本文件内部左侧的文件树中找到我们想要的文件目录。</li></ul><img src="/2022/04/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/1.png" class><h2 id="数据的相关分析">数据的相关分析</h2><p><strong>相关分析</strong>（correlation analysis）后我们可以通过相关性系数了解数据集中任意一对变量(a, b)间的相关性。相关性系数是一个-1~1的值，正值表示正相关的，负值表示负相关，如果a和b的相关系数是1，则a和b总是相等的。<br>在python中我们可以用<strong>热力图</strong>(heatmap)的方式非常直观的地展示出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns <span class="comment"># Seaborn为统计学数据可视化工具库</span></span><br><span class="line">sns.heatmap(df_ads.corr(), cmap=<span class="string">&#x27;YlGnBu&#x27;</span>, annot=<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/2022/04/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/2.png" class><ul><li>从热力图结果来看，将资金投放在微信上对销售额影响最大，因此应该多投资微信广告</li></ul><h2 id="数据散点图">数据散点图</h2><p>通过散点图（scatter plot）两两一组显示商品销售额和各种广告投放金额之间的对应关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示销售额和各种广告投放金额的散点图</span></span><br><span class="line">sns.pairplot(df_ads,</span><br><span class="line">                x_vars=[<span class="string">&#x27;wechat&#x27;</span>,<span class="string">&#x27;weibo&#x27;</span>,<span class="string">&#x27;others&#x27;</span>],y_vars=<span class="string">&#x27;sales&#x27;</span>,</span><br><span class="line">                height=<span class="number">4</span>,aspect=<span class="number">1</span>,kind=<span class="string">&#x27;scatter&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/2022/04/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/3.png" class><ul><li>从这个图中我们就可以看出销售额和各种广告投资间大概的函数关系。</li></ul><h2 id="数据清洗和规范化">数据清洗和规范化</h2><blockquote><p>数据清洗是为了去除掉误差很大的数据或者我们不需要的数据。<br>规范化是为了让后续的数据更好被处理，以及更容易被人理解。</p></blockquote><p>下面的代码则是保留了微信相关的数据集，删去了微博和其他这两个特征段。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = np.array(df_ads.wechat)</span><br><span class="line">y = np.array(df_ads.sales)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量X的阶：&quot;</span>, X.ndim)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量X的形状：&quot;</span>, X.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量X的内容：\n&quot;</span>, X)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">张量X的阶： 1</span><br><span class="line">X的形状： (200,)</span><br><span class="line">X的内容：</span><br><span class="line"> [ 304.4 1011.9 1091.1   85.5 1047.   940.9 1277.2   38.2  342.6  347.6</span><br><span class="line">  980.1   39.1   39.6  889.1  633.8  527.8  203.4  499.6  633.4  437.7</span><br><span class="line">  334.  1132.   841.3  435.4  627.4  599.2  321.2  571.9  758.9  799.4</span><br><span class="line">  314.   108.3  339.9  619.7  227.5  347.2  774.4 1003.3   60.1   88.3</span><br><span class="line"> 1280.4  743.9  805.4  905.    76.9 1088.8  670.2  513.7 1067.    89.2</span><br><span class="line">  130.1  113.8  195.7 1000.1  283.5 1245.3  681.1  341.7  743.   976.9</span><br><span class="line"> 1308.6  953.7 1196.2  488.7 1027.4  830.8  984.6  143.3 1092.5  993.7</span><br><span class="line"> 1290.4  638.4  355.8  854.5    3.2  615.2   53.2  401.8 1348.6   78.3</span><br><span class="line"> 1188.9 1206.7  899.1  364.9  854.9 1099.7  909.1 1293.6  311.2  411.3</span><br><span class="line">  881.3 1091.5   18.7  921.4 1214.4 1038.8  427.2  116.5  879.1  971.</span><br><span class="line">  899.1  114.2   78.3   59.6  748.5  681.6  261.6 1083.8 1322.7  753.5</span><br><span class="line"> 1259.9 1080.2   33.2  909.1 1092.5 1208.5  766.2  467.3  611.1  202.5</span><br><span class="line">   24.6  442.3 1301.3  314.9  634.7  408.1  560.1  503.7 1154.8 1130.2</span><br><span class="line">  932.8  958.7 1044.2 1274.9  550.6 1259.   196.1  548.3  650.2   81.4</span><br><span class="line">  499.6 1033.8  219.8  971.4  779.4 1019.2 1141.6  994.2  986.4 1318.1</span><br><span class="line">  300.8  588.8 1056.1  179.7 1080.2  255.7 1011.9  941.4  928.7  167.9</span><br><span class="line">  271.2  822.6 1162.1  596.5  990.5  533.3 1335.9  308.5 1106.6  805.4</span><br><span class="line"> 1002.4  347.6  443.6  389.9  642.9  243.4  841.3   35.5   85.1  784.9</span><br><span class="line">  428.6  173.8 1037.4  712.5  172.9  456.8  396.8 1332.7  546.9  857.2</span><br><span class="line">  905.9  475.9  959.1  125.1  689.3  869.5 1195.3  121.9  343.5  796.7]</span><br></pre></td></tr></table></figure><ul><li><strong>对于回归问题的数值类型数据集，机器学习模型所读入的规范格式应该为2D张量，即矩阵，其形状为(样本数，标签数)</strong>，因此我们下一步需要给张量变形。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = X.reshape(<span class="built_in">len</span>(X), <span class="number">1</span>)</span><br><span class="line">y = y.reshape(<span class="built_in">len</span>(y), <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量X的阶：&quot;</span>, X.ndim)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量X的形状：&quot;</span>, X.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量X的内容：\n&quot;</span>, X)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">张量X的阶： 2</span><br><span class="line">X的形状： (200, 1)</span><br><span class="line">X的内容：</span><br><span class="line"> [[ 304.4]</span><br><span class="line"> [1011.9]</span><br><span class="line"> [1091.1]</span><br><span class="line"> ...</span><br><span class="line"> [ 121.9]</span><br><span class="line"> [ 343.5]</span><br><span class="line"> [ 796.7]]</span><br></pre></td></tr></table></figure><h2 id="拆分数据集为训练集和测试集">拆分数据集为训练集和测试集</h2><p>在普通的机器学习项目中，至少需要包含这两个数据集，一个用于训练机器，确定模型，另一个用于测试模型的准确性。不仅如此，往往还需要一个验证集，以在最终测试之前增加验证环节。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将数据集进行80%（训练集）/20%（测试集）的分割</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size = <span class="number">0.2</span>, random_state = <span class="number">0</span>)</span><br></pre></td></tr></table></figure><ul><li>test_size = 0.2，表示拆分出来的测试集占总样本量的20%</li><li>train_test_split工具已经将数据集打乱顺序了</li><li>random_state用于数据集拆分过程的随机化设定。如果指定了一个整数，那么它叫做随机化种子</li></ul><h2 id="数据归一化">数据归一化</h2><p>归一化是按比例的线性缩放。数据归一化之后，数据分布不变，但是都落在一个特定区间，比如(-1, 1)或者(0, 1)<br>常见归一化公式如下：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">x&#x27; = \cfrac{x-min(x)}{max(x) - min(x)} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8019em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.526em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">min</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">min</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span></span></span></span></span></span></span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scalar</span>(<span class="params">train, test</span>)</span><br><span class="line"><span class="built_in">min</span> = train.<span class="built_in">min</span>(axis = <span class="number">0</span>)</span><br><span class="line"><span class="built_in">max</span> = train.<span class="built_in">max</span>(axis = <span class="number">0</span>)</span><br><span class="line">gap = <span class="built_in">max</span> - <span class="built_in">min</span></span><br><span class="line">train -= <span class="built_in">min</span></span><br><span class="line">train /= gap</span><br><span class="line">test -= <span class="built_in">min</span></span><br><span class="line">test /= gap</span><br><span class="line"><span class="keyword">return</span> train, test</span><br></pre></td></tr></table></figure><ul><li>归一化过程中的最大值、最小值、差都来自于训练集。<strong>不能使用测试集的数据进行特征缩放</strong>。因此，可能会出现测试集中最小值比训练集小，最大值比训练集大的情况。</li><li>这样做的原因是我们把测试集当作外来数据，我们在训练数据的时候，测试集还没出现呢。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 归一化</span></span><br><span class="line">X_train, X_test = scalar(X_train, X_test)</span><br><span class="line">y_train, y_test = scalar(y_train, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示散点图</span></span><br><span class="line">plt.plot(X_train, <span class="string">&#x27;r.&#x27;</span>, label=<span class="string">&#x27;Training data&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;wechat&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;sales&#x27;</span>)</span><br><span class="line">plt.legend() <span class="comment"># 显示图例</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/2022/04/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/4.png" class><h1>选择机器学习模型</h1><h2 id="确定线性回归模型">确定线性回归模型</h2><p>对于此例，我们选择一元线性函数模型——<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>a</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = ax + b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span>.<br>不过在机器学习中我们习惯性写作 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>w</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = wx + b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span><br>w表示weight(权重)，b表示bias（偏置）。也有一些地方写成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><msub><mi>θ</mi><mn>0</mn></msub><mi>x</mi><mo>+</mo><msub><mi>θ</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">y = \theta_0x + \theta_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p><h2 id="假设（预测）函数——h-x">假设（预测）函数——h(x)</h2><p>确定以线性函数作为机器学习的模型之后，我们需要介绍以下假设函数的概念。先来看一个与线性函数稍有差别的方程式：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>y</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mi>w</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y&#x27; = wx + b </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9963em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></span></p><p>或者写成:</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>w</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">h(x) = wx + b </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></span></p><ul><li>y’指的是所预测出的标签，读作y帽（y-hat）或y撇。</li><li>h(x)就是机器学习所得到的函数模型，它能根据输入的特征进行标签的预测。我们把它称为<strong>假设函数</strong>(hypothesis function)。</li></ul><p>因为我们遇到实际问题的时候，可能不能像这个例子一样马上就确定我们要用一次函数作为模型，所以我们现在把这个例子想得复杂点，这个时候，我们就只能猜：哼~也许一次函数挺好用的吧。所以我们把这个一次函数称作假设函数。</p><h2 id="损失（误差）函数——L-w-b">损失（误差）函数——L(w, b)</h2><p>在假定了假设函数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>w</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y=wx+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span>之后，我们的目标就是找到w,b的具体值。那怎么确定这两个值呢？我们需要引入<strong>损失</strong>（loss）的概念。（最简单的想法就是，自己找两个数带进去，然后判断好不好，拟合模型误差大不大呗）</p><p><strong>损失，是对糟糕预测的惩罚</strong>。损失就是<strong>误差</strong>，也称为<strong>成本代价</strong>。如果模型的预测完全准确，那损失就是0。我们的目标就是0，当然，没损失几乎不可能咯。<br><strong>损失函数</strong>(loss function)<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(w, b)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span></span></span></span>就是用来计算平均损失的。（有些地方把损失函数记作<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">j(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span>，称作<strong>代价函数、成本函数</strong>）<br>如果平均损失很小，参数就好；反之则差，还需继续调整。这个计算当前假设函数所造成的损失的过程，就是前面提到过的<strong>模型内部参数的评估</strong>的过程。<br>机器学习中的损失函数，主要包括以下几种：</p><blockquote><p>用于回归的损失函数：</p><blockquote><p>均方误差(Mean Square Error, MSE)函数，也叫平方损失或L2损失函数。<br>平均绝对误差(Mean Absolute Error, MAE)函数，也叫L1损失函数。<br>平均偏差误差(Mean Bias Error, MBE)函数。</p></blockquote></blockquote><blockquote><p>用于分类的损失函数：</p><blockquote><p>交叉熵损失(cross-entropy loss)函数。<br>多分类SVM损失(hinge loss)函数。</p></blockquote></blockquote><h2 id="均方误差函数">均方误差函数</h2><ul><li>首先，对于每一个样本，其预测值和真实值的差异为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><msup><mi>y</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(y - y&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，而<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>y</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mi>w</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y&#x27;=wx+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9463em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span>，所以损失值与参数w和b有关。</li><li>如果将损失值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><msup><mi>y</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(y-y&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>夸张一下，进行平方，将差值正化，变成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><msup><mi>y</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">(y-y&#x27;)^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>。我们把这个值叫做单个样本的平方损失。</li><li>然后，需要把所有样本的平方损失相加，即<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>−</mo><msup><mi>y</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>−</mo><msup><mi>y</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mo separator="true">⋅</mo><mo separator="true">⋅</mo><mo separator="true">⋅</mo><mo>+</mo><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>200</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>−</mo><msup><mi>y</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>200</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">(y(x^{(1)})-y&#x27;(x^{(1)}))^2+(y(x^{(2)})-y&#x27;(x^{(2)}))^2+···+(y(x^{(200)})-y&#x27;(x^{(200)}))^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord">+</span><span class="mpunct">⋅⋅⋅</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">+</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">200</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">200</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><br>写成求和的形式：</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><munder><mo>∑</mo><mstyle scriptlevel="1"><mtable rowspacing="0.1em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="1" displaystyle="false"><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>D</mi></mrow></mstyle></mtd></mtr></mtable></mstyle></munder><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex"> \sum_{    \begin{subarray}{}        (x,y) \in D     \end{subarray}}(y-h(x))^2 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.566em;vertical-align:-1.516em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.809em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.75em;"><span style="top:-2.75em;"><span class="pstrut" style="height:2.75em;"></span><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25em;"><span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li>最后，根据样本的数量求平均值，则损失函数为：</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mi>M</mi><mi>S</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>N</mi></mrow></mfrac><munder><mo>∑</mo><mstyle scriptlevel="1"><mtable rowspacing="0.1em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="1" displaystyle="false"><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>D</mi></mrow></mstyle></mtd></mtr></mtable></mstyle></munder><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex"> L(w,b)=MSE=\cfrac{1}{2N} \sum_{    \begin{subarray}{}    (x,y) \in D     \end{subarray}}     (y-h(x))^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">MSE</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.516em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.809em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.75em;"><span style="top:-2.75em;"><span class="pstrut" style="height:2.75em;"></span><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25em;"><span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p><p>关于以上公式，说明以上几点：</p><ul><li>(x, y)为样本，x是特征，y是标签</li><li>h(x)是假设函数wx+b，也就是y’</li><li>D是包含多个样本的数据集</li><li>N指的是样本数量（此例中是200）。N前面还有一个常量2，是为了在求梯度的时候，抵消二次方后产生的系数，方便后续进行计算，同时增加的这个常量并不影响梯度下降的最小结果。</li><li>L是权重w和偏置b的函数，它的大小随着w和b的变化而变化。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss_function</span>(<span class="params">X, y, weight, bias</span>):</span><br><span class="line">    y_hat = weight * X + bias</span><br><span class="line">    loss = y - y_hat</span><br><span class="line">    cost = np.<span class="built_in">sum</span>(loss**<span class="number">2</span>)/(<span class="number">2</span>*<span class="built_in">len</span>(X))</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;当权重为5，偏置为3时，损失为：&quot;</span>, loss_function(X_train, y_train, weight=<span class="number">5</span>, bias=<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;当权重为100，偏置为1时，损失为：&quot;</span>, loss_function(X_train, y_train, weight=<span class="number">100</span>, bias=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">当权重为5，偏置为3时，损失为： 12.796390970780058</span><br><span class="line">当权重为100，偏置为1时，损失为： 1577.9592615030556</span><br></pre></td></tr></table></figure><ul><li>因此我们可以说，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>3</mn><mi>x</mi><mo>+</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">y=3x+5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">3</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">5</span></span></span></span>比<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>100</mn><mi>x</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y=100x+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">100</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>是更优秀的模型。</li></ul><blockquote><p>为什么说我们要用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><msup><mi>y</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">(y-y&#x27;)^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>呢，取绝对值不香嘛？反正只要求正数和的平均值就好了。<strong>之所以平方，是为了让L(w, b)形成相对于w和b的凸函数，从而实现梯度下降</strong></p></blockquote><h2 id="通过梯度下降找到最佳参数">通过梯度下降找到最佳参数</h2><p>上部分我们自己定义了两组参数，明眼人，学过数学的，都能看出来(3,5)要比(100,1)好吧！但是机器就是呆呆的，只能通过损失函数判断。那我们要怎么去找一组最理想的参数(w,b)呢？显然，利用电脑算的比人快不知道多少的优点，我们可以让机器随机生成一亿组参数，每组都算一下loss，然后从小到大排一下，不就找到了一亿组参数中效果最好的参数了嘛，大差不差也算找到了。<br>简单吧，好吧，不开玩笑了，虽然说这样做<strong>也不是不行</strong>，但是这显然有点low，而且计算量大，毫无目的性可言，就像无头苍蝇到处乱撞。<strong>因此，理想的情况是，每一次生成参数、猜测都应该比上一次要好，我们让参数(w,b)逐渐变好不就行了嘛</strong>，那么怎么做呢，这就要用到<strong>梯度下降</strong>了。</p><h2 id="凸函数确保有最小损失点">凸函数确保有最小损失点</h2><p>梯度下降就是我们寻找最优参数的<strong>方向</strong>，而光有方向还不够，我们还需要知道什么时候停止。<br>回忆一下均方差函数：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mi>M</mi><mi>S</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>N</mi></mrow></mfrac><munder><mo>∑</mo><mstyle scriptlevel="1"><mtable rowspacing="0.1em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="1" displaystyle="false"><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>D</mi></mrow></mstyle></mtd></mtr></mtable></mstyle></munder><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><mo stretchy="false">(</mo><mi>w</mi><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex"> L(w,b)=MSE=\cfrac{1}{2N} \sum_{    \begin{subarray}{}    (x,y) \in D     \end{subarray}}     (y-(wx+b))^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">MSE</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.516em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.809em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.75em;"><span style="top:-2.75em;"><span class="pstrut" style="height:2.75em;"></span><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25em;"><span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p><blockquote><p>这个函数的图像大概如下：</p></blockquote><img src="/2022/04/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/5.png" class><blockquote><p>我们将这个图像称为损失曲线，是一个凸函数，其存在一个<strong>全局最小损失点</strong></p></blockquote><h2 id="梯度下降的实现">梯度下降的实现</h2><p>例如此时我们要找w的最优值，而现在w=5,接下来我们要让w=5.01还是4.99呢，当然，如果你看到了函数图像，你可能就知道它要向数轴哪边移动了。如果我们看不到图像，怎么办呢？这就需要我们高数中的知识了：求导！<br>对当前点的位置求导：</p><ul><li>如果求导之后梯度为正值，则说明L正在随着w增大而增大，应该减小w</li><li>如果求导之后梯度为负值，则说明L正在随着w增大而减小，应该增大w</li></ul><p>用数学语言描述如下：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>梯度</mtext><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi mathvariant="normal">∂</mi><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mstyle><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mrow><mn>2</mn><mi>N</mi></mrow></mfrac></mstyle><msub><mo>∑</mo><mstyle scriptlevel="1"><mtable rowspacing="0.1em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="1" displaystyle="false"><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>D</mi></mrow></mstyle></mtd></mtr></mtable></mstyle></msub><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><mo stretchy="false">(</mo><mi>w</mi><mo>⋅</mo><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><msub><mo>∑</mo><mstyle scriptlevel="1"><mtable rowspacing="0.1em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="1" displaystyle="false"><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>D</mi></mrow></mstyle></mtd></mtr></mtable></mstyle></msub><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mi>w</mi><mo>⋅</mo><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>x</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi mathvariant="normal">∂</mi><mrow><mi mathvariant="normal">∂</mi><mi>b</mi></mrow></mfrac></mstyle><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mrow><mn>2</mn><mi>N</mi></mrow></mfrac></mstyle><msub><mo>∑</mo><mstyle scriptlevel="1"><mtable rowspacing="0.1em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="1" displaystyle="false"><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>D</mi></mrow></mstyle></mtd></mtr></mtable></mstyle></msub><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><mo stretchy="false">(</mo><mi>w</mi><mo>⋅</mo><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><msub><mo>∑</mo><mstyle scriptlevel="1"><mtable rowspacing="0.1em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="1" displaystyle="false"><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>D</mi></mrow></mstyle></mtd></mtr></mtable></mstyle></msub><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mi>w</mi><mo>⋅</mo><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex"> 梯度= \begin{cases}\cfrac{\partial}{\partial w}\cfrac{1}{2N}\sum_{    \begin{subarray}{}    (x,y) \in D     \end{subarray}}     (y-(w \cdot x+b))^2 =    \cfrac{1}{N}\sum_{    \begin{subarray}{}    (x,y) \in D     \end{subarray}}     ((w \cdot x+b)-y)\cdot x \\    \\\cfrac{\partial}{\partial b}\cfrac{1}{2N}\sum_{    \begin{subarray}{}    (x,y) \in D     \end{subarray}}     (y-(w \cdot x+b))^2 =    \cfrac{1}{N}\sum_{    \begin{subarray}{}    (x,y) \in D     \end{subarray}}     ((w \cdot x+b)-y) \\\end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">梯度</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:6em;vertical-align:-2.75em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.25em;"><span style="top:-1.366em;"><span class="pstrut" style="height:3.216em;"></span><span class="delimsizinginner delim-size4"><span>⎩</span></span></span><span style="top:-1.358em;"><span class="pstrut" style="height:3.216em;"></span><span style="height:1.216em;width:0.8889em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.8889em" height="1.216em" style="width:0.8889em" viewbox="0 0 888.89 1216" preserveaspectratio="xMinYMin"><path d="M384 0 H504 V1216 H384z M384 0 H504 V1216 H384z"/></svg></span></span><span style="top:-3.216em;"><span class="pstrut" style="height:3.216em;"></span><span class="delimsizinginner delim-size4"><span>⎨</span></span></span><span style="top:-4.358em;"><span class="pstrut" style="height:3.216em;"></span><span style="height:1.216em;width:0.8889em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.8889em" height="1.216em" style="width:0.8889em" viewbox="0 0 888.89 1216" preserveaspectratio="xMinYMin"><path d="M384 0 H504 V1216 H384z M384 0 H504 V1216 H384z"/></svg></span></span><span style="top:-5.566em;"><span class="pstrut" style="height:3.216em;"></span><span class="delimsizinginner delim-size4"><span>⎧</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.75em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.246em;"><span style="top:-5.246em;"><span class="pstrut" style="height:3.59em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2253em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.75em;"><span style="top:-2.75em;"><span class="pstrut" style="height:2.75em;"></span><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25em;"><span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4747em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2253em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.75em;"><span style="top:-2.75em;"><span class="pstrut" style="height:2.75em;"></span><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25em;"><span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4747em;"><span></span></span></span></span></span></span><span class="mopen">((</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">x</span></span></span><span style="top:-3.552em;"><span class="pstrut" style="height:3.59em;"></span><span class="mord"></span></span><span style="top:-1.53em;"><span class="pstrut" style="height:3.59em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">b</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2253em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.75em;"><span style="top:-2.75em;"><span class="pstrut" style="height:2.75em;"></span><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25em;"><span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4747em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2253em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.75em;"><span style="top:-2.75em;"><span class="pstrut" style="height:2.75em;"></span><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25em;"><span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4747em;"><span></span></span></span></span></span></span><span class="mopen">((</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.746em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>利用代码写出公式（这段代码不用写在notebook中）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y_hat = weight*X+bias</span><br><span class="line">loss = y_hat-y</span><br><span class="line">derivative_weight = X.T.dot(loss)/<span class="built_in">len</span>(X) <span class="comment"># 对权重求导</span></span><br><span class="line">derivative_bias = <span class="built_in">sum</span>(loss)*<span class="number">1</span>/<span class="built_in">len</span>(X) <span class="comment"># 对偏置求导</span></span><br></pre></td></tr></table></figure><ul><li>对偏置b求导并不需要与特征X相乘，因为偏置与权重不同，它与特征并不相关。另外还有一种思路，是把偏置看作<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">w_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，那么就需要X特征矩阵添加一行数字1，形成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">X_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，与偏置相乘，同时确保偏置值不变。</li></ul><h2 id="确保学习速率">确保学习速率</h2><p>学习速率就是指以多快的速度下山。学习速率(learning rate)也记作<strong>α</strong>。<br>学习速率乘以损失曲线求导后的微分值，就是梯度变化的步长。它控制着当前梯度下降的节奏，w将在每一次迭代过程中被更新、优化。<br>w按照如下方式更新：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>w</mi><mo>=</mo><mi>w</mi><mo>−</mo><mi>α</mi><mo>⋅</mo><mfrac><mi mathvariant="normal">∂</mi><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac><mi>L</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w=w-\alpha \cdot \cfrac{\partial}{\partial w}L(w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4445em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.276em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span></span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weight = weight - alpha * derivative_weight</span><br><span class="line">bias = bias - alpha * derivative_bias</span><br></pre></td></tr></table></figure><ul><li>α是一个<strong>超参数</strong></li><li>显然，如果α太小，机器学习的速度会很慢，如果α太大，可能会容易越过最低点。</li></ul><p>下面给出梯度下降的完整代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">X, y, w, b, lr, <span class="built_in">iter</span></span>):</span><br><span class="line">    l_history = np.zeros(<span class="built_in">iter</span>) <span class="comment"># 初始化记录梯度下降过程中损失的数组</span></span><br><span class="line">    w_history = np.zeros(<span class="built_in">iter</span>) <span class="comment"># 初始化记录梯度下降过程中权重的数组</span></span><br><span class="line">    b_history = np.zeros(<span class="built_in">iter</span>) <span class="comment"># 初始化记录梯度下降过程中偏置的数组</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">iter</span>):</span><br><span class="line">        y_hat = w * X + b</span><br><span class="line">        loss = y_hat - y</span><br><span class="line">        derivative_w = X.T.dot(loss)/<span class="built_in">len</span>(X)</span><br><span class="line">        derivative_b = <span class="built_in">sum</span>(loss)*<span class="number">1</span>/<span class="built_in">len</span>(X)</span><br><span class="line">        w = w - lr * derivative_w</span><br><span class="line">        b = b - lr * derivative_b</span><br><span class="line">        l_history = loss_function(X, y, w, b)</span><br><span class="line">        w_history = w</span><br><span class="line">        b_history = b</span><br><span class="line">    <span class="keyword">return</span> l_history, w_history, b_history</span><br></pre></td></tr></table></figure><h1>实现一元线性回归模型并调试超参数</h1><h2 id="设定初始值">设定初始值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">iteration = <span class="number">100</span> <span class="comment"># 迭代100次</span></span><br><span class="line">alpha = <span class="number">1</span> <span class="comment"># 初始学习速率为1</span></span><br><span class="line">weight = -<span class="number">5</span></span><br><span class="line">bias = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算初始权重和偏置带来的损失</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;当前损失：&#x27;</span>, loss_function(X_train, y_train, weight, bias))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当前损失： 1.343795534906634</span></span><br></pre></td></tr></table></figure><p>下面画出当前回归函数的图像：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(X_train, y_train, <span class="string">&#x27;r.&#x27;</span>, label=<span class="string">&#x27;Training data&#x27;</span>)</span><br><span class="line">line_X = np.linspace(X_train.<span class="built_in">min</span>(), X_train.<span class="built_in">max</span>(), <span class="number">500</span>)</span><br><span class="line">line_y = [weight * xx + bias <span class="keyword">for</span> xx <span class="keyword">in</span> line_X]</span><br><span class="line">plt.plot(line_X, line_y, <span class="string">&#x27;b--&#x27;</span>, label=<span class="string">&#x27;Current hypothesis&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;wechat&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;sales&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>输出图像如下所示：</p><img src="/2022/04/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/6.png" class><ul><li>将初始假设函数设定如此“离谱”，是为了让人直观看到后面梯度下降的过程和变化。</li></ul><h2 id="进行梯度下降">进行梯度下降</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据初始参数值，进行梯度下降。</span></span><br><span class="line">loss_history, weight_history, bias_history = gradient_descent(X_train, y_train, weight, bias, alpha, iteration)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制损失曲线</span></span><br><span class="line">plt.plot(loss_history, <span class="string">&#x27;g--&#x27;</span>, label=<span class="string">&#x27;Loss Curve&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Iteration&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>如图：</p><img src="/2022/04/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/7.png" class><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制当前的函数模型</span></span><br><span class="line">plt.plot(X_train, y_train, <span class="string">&#x27;r.&#x27;</span>, label=<span class="string">&#x27;Training data&#x27;</span>)</span><br><span class="line">line_X=np.linspace(X_train.<span class="built_in">min</span>(), y_train.<span class="built_in">min</span>(), <span class="number">500</span>)</span><br><span class="line">line_y = [weight_history[-<span class="number">1</span>] * xx + bias_history[-<span class="number">1</span>] <span class="keyword">for</span> xx <span class="keyword">in</span> line_X]</span><br><span class="line">plt.plot(line_X, line_y, <span class="string">&#x27;b--&#x27;</span>, label=<span class="string">&#x27;Current hypothesis&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;wechat&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;sales&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>如图：</p><img src="/2022/04/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/8.png" class><h2 id="调试超参数">调试超参数</h2><p>超参数作为函数外部的参数，也是可以很大程度上影响学习结果的。显而易见，迭代次数越多，效果就越来越好，但是可能由于边际效益，后面的训练可能也没什么必要了。<br>这个例子中主要有两个影响学习效果的超参数：迭代次数iteration和学习速率α。<br>虽然说从展示的图片来看，我们初始设定的这两个参数的值已经挺不错的了，但大家也可以去调调看，这样可以对机器学习的模型有更深刻的理解。这里仅给出几个例子：</p><blockquote><p>1 α=0.5，iteration=100时:</p></blockquote><img src="/2022/04/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/9.png" class><blockquote><p>2 α=1，iteration=200时:</p></blockquote><img src="/2022/04/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/10.png" class><p>下面输出α=1，iteration=100时的参数值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;当前损失：&quot;</span>,loss_function(X_train, y_train,</span><br><span class="line">                    weight_history[-<span class="number">1</span>], bias_history[-<span class="number">1</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;当前权重：&quot;</span>, weight_history[-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;当前偏置：&quot;</span>, bias_history[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">当前损失： 0.00465780405531404</span><br><span class="line">当前权重： 0.6552253409192808</span><br><span class="line">当前偏置： 0.17690341009472488</span><br></pre></td></tr></table></figure><h2 id="在测试集上进行预测">在测试集上进行预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试集损失：&quot;</span>, loss_function(X_test, y_test,</span><br><span class="line">                        weight_history[-<span class="number">1</span>], bias_history[-<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集损失：0.00458180938024721</span></span><br></pre></td></tr></table></figure><ul><li>损失很小，效果不错，甚至好过训练集。</li></ul><p>说明我们找到了一个比较好的模型！</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>w</mi><mo>=</mo><mn>0.6552253409192808</mn><mspace linebreak="newline"></mspace><mi>b</mi><mo>=</mo><mn>0.17690341009472488</mn><mspace linebreak="newline"></mspace><mtext>模型：</mtext><msup><mi>y</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mn>0.66</mn><mi>x</mi><mo>+</mo><mn>0.17</mn></mrow><annotation encoding="application/x-tex">w = 0.6552253409192808 \\ b = 0.17690341009472488 \\ 模型：y&#x27;=0.66x+0.17</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.6552253409192808</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.17690341009472488</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.9963em;vertical-align:-0.1944em;"></span><span class="mord cjk_fallback">模型：</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">0.66</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.17</span></span></span></span></span></p><p>我们还可以同时显示测试集与训练集的损失曲线。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同时显示测试集与训练集的损失曲线</span></span><br><span class="line">loss_history_tr, weight_history_tr, bias_history_tr = gradient_descent(X_train, y_train, weight, bias, alpha, iteration)</span><br><span class="line">loss_history_te, weight_history_te, bias_history_te = gradient_descent(X_test, y_test, weight, bias, alpha, iteration)</span><br><span class="line">plt.plot(loss_history_tr, <span class="string">&#x27;b--&#x27;</span>, label=<span class="string">&#x27;Training Loss Curve&#x27;</span>)</span><br><span class="line">plt.plot(loss_history_te, <span class="string">&#x27;g-&#x27;</span>, label=<span class="string">&#x27;Testing Loss Curve&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Iteration&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>如图：</p><img src="/2022/04/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/11.png" class><blockquote><p>由于这一章内容较多，剩下的内容放在下一节中，由于有关线性回归的内容实际上已经比较完整了，在此小作暂停。</p></blockquote><img src="/2022/04/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/%E5%8F%AF%E7%88%B1.jpg" class>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;线性回归——预测网店的销售额&lt;/h1&gt;
&lt;h1&gt;问题定义&lt;/h1&gt;
&lt;p&gt;张三在运营一个网店，他发现网店商品的销量和广告推广的力度息息相关。张三准备好了以下几个问题亟待解决：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（1）各种广告和商品销售额的相关度如何？&lt;/li&gt;
&lt;li&gt;（2）各种</summary>
      
    
    
    
    <category term="人工智能学习路线" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    <category term="从零开始机器学习" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="人工智能" scheme="https://yigexiaogai.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="机器学习，毕设相关" scheme="https://yigexiaogai.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E6%AF%95%E8%AE%BE%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战（二）</title>
    <link href="https://yigexiaogai.github.io/2022/04/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>https://yigexiaogai.github.io/2022/04/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/</id>
    <published>2022-04-05T05:08:44.000Z</published>
    <updated>2022-06-04T14:02:02.791Z</updated>
    
    <content type="html"><![CDATA[<h1>基础数学知识</h1><h1>机器学习中的函数</h1><p>**机器学习基本上等价于寻找函数的过程。**机器学习的目的是进行预测、判断，实现某种功能。通过学习训练集中的数据，计算机得到一个从x到y的拟合结果，也就是函数。例如：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>x</mi><mtext>成本</mtext></msub><mo>→</mo><mn>100</mn><mtext>万</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>x</mi><mtext>演员</mtext></msub><mo>→</mo><mtext>大明星</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>x</mi><mtext>广告</mtext></msub><mo>→</mo><mn>200</mn><mtext>万</mtext></mrow></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow><mo>=</mo><mn>1</mn><mtext>亿元</mtext></mrow><annotation encoding="application/x-tex"> f(x)\begin{pmatrix}   x_{成本} \to 100万\\   x_{演员} \to 大明星\\   x_{广告} \to 200万\end{pmatrix} = 1亿元 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.6em;vertical-align:-1.55em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-2.25em;"><span class="pstrut" style="height:3.155em;"></span><span class="delimsizinginner delim-size4"><span>⎝</span></span></span><span style="top:-3.397em;"><span class="pstrut" style="height:3.155em;"></span><span style="height:0.016em;width:0.875em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.875em" height="0.016em" style="width:0.875em" viewbox="0 0 875 16" preserveaspectratio="xMinYMin"><path d="M291 0 H417 V16 H291z M291 0 H417 V16 H291z"/></svg></span></span><span style="top:-4.05em;"><span class="pstrut" style="height:3.155em;"></span><span class="delimsizinginner delim-size4"><span>⎛</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord cjk_fallback mtight">成本</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">100</span><span class="mord cjk_fallback">万</span></span></span><span style="top:-3.01em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord cjk_fallback mtight">演员</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord cjk_fallback">大明星</span></span></span><span style="top:-1.81em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord cjk_fallback mtight">广告</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">200</span><span class="mord cjk_fallback">万</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-2.25em;"><span class="pstrut" style="height:3.155em;"></span><span class="delimsizinginner delim-size4"><span>⎠</span></span></span><span style="top:-3.397em;"><span class="pstrut" style="height:3.155em;"></span><span style="height:0.016em;width:0.875em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.875em" height="0.016em" style="width:0.875em" viewbox="0 0 875 16" preserveaspectratio="xMinYMin"><path d="M457 0 H583 V16 H457z M457 0 H583 V16 H457z"/></svg></span></span><span style="top:-4.05em;"><span class="pstrut" style="height:3.155em;"></span><span class="delimsizinginner delim-size4"><span>⎞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">1</span><span class="mord cjk_fallback">亿元</span></span></span></span></span></p><p>除了线性函数（一次函数）、二次函数、多次函数和对数函数以外，机器学习中有一种很重要的函数：激活函数。</p><h2 id="激活函数（activation-function）">激活函数（activation function）</h2><p>激活函数的作用是在学习算法中实现非线性的、阶跃性质的变换，其中最常见的是Sigmoid函数。</p><img src="/2022/04/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/sigmoid.jpg" class title="sigmoid函数"><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>y</mi><mo>=</mo><mn>1</mn><mo stretchy="false">(</mo><mi>x</mi><mo>&gt;</mo><mn>0</mn><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mi>y</mi><mo>=</mo><mn>0</mn><mo stretchy="false">(</mo><mi>x</mi><mo>&lt;</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y=1(x&gt;0) \\ y=0(x&lt;0) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">0</span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">0</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">0</span><span class="mclose">)</span></span></span></span></span></p><h2 id="函数凹凸性">函数凹凸性</h2><p>凸函数是只存在一个最低点的函数。例如如下图：</p><img src="/2022/04/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/%E5%87%B8%E5%87%BD%E6%95%B0.png" class><blockquote><p>注意：在各种地方对于凸函数和凹函数的定义不同，方向可能会相反，但是因为函数概念和机器学习概念最初都是国外来的，因此我们遵循国外的习惯，即：函数长这样 U 叫做凸函数。</p></blockquote><p>想象有一个小球在凸函数上滚动，它最后就会滚到最低点，别的函数都做不到。<strong>达到全局最低点</strong>就是我们在机器学习中理想的情况。</p><h1>梯度下降</h1><h2 id="什么是梯度？">什么是梯度？</h2><p>对多元函数的各函数求偏导，然后把所求的各个参数的偏导数以向量形式写出来，就是梯度。<br>计算梯度向量的意义在于其函数变化的方向，而且是变化最快的方向。</p><h2 id="下山的隐喻">下山的隐喻</h2><p>在机器学习中用下山来比喻梯度下降是很常见的。想象一个人在山上，想知道如何下山，只能一步一步往下走，在一个位置求这个位置的梯度，然后沿着梯度的负方向也就是最陡峭的地方向下走一步，然后再求新位置的梯度，循环往复，直到走到山脚。如果此时这座山不是凸函数的话，也许这个人在山腰处就卡住了，以为那里就是山脚。（局部最优解）<br>梯度下降的用处。简单的说，有以下几点：</p><ul><li>机器学习的本质是找到最优的函数。</li><li>如何衡量函数是否优化？其方法是尽量减少预测值和真值之间的误差（损失值）</li><li>可以建立误差和模型参数之间的函数（最好是凸函数）</li><li>梯度下降能够引导我们走到凸函数的全局最低点，也就是误差最少的时候。</li></ul><h1>机器学习的数据结构——张量（tensor）</h1><h2 id="张量的轴、阶、形状">张量的轴、阶、形状</h2><p>这一块内容非常重要，因为在机器学习中，我们常常会接触到张量。<br><strong>张量是机器学习程序中的数字容器，本质上就是各种不同维度的数组。张量的维度称之为轴（axis），轴的个数称为阶（rank）（阶就是俗语的维度，但是请务必注意其不同）。</strong><br>张量的<strong>形状</strong>（shape）就是张量的阶，加上每个阶的维度。</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><mo>=</mo><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mo>+</mo><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi mathvariant="normal">.</mi><mi>l</mi><mi>e</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">shape = rank +  rank.len </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">ha</span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.03148em;">ank</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.03148em;">ank</span><span class="mord">.</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span></span></span></span></span></p><p>这是我对于张量的理解，阶的维度其实就是阶的长度。</p><h2 id="标量——0D（阶）张量">标量——0D（阶）张量</h2><p>仅包含一个数字的张量叫<strong>标量</strong>（scalar）。<br>利用numpy创建一个标量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 后续代码省略导入numpy库</span></span><br><span class="line">X = np.array(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的值：&quot;</span>, X)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的阶&quot;</span>, X.ndim)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的数据类型&quot;</span>, X.dtype)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的形状&quot;</span>, X.shape)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X的值： 5</span><br><span class="line">X的阶 0</span><br><span class="line">X的数据类型 int64</span><br><span class="line">X的形状 ()</span><br></pre></td></tr></table></figure><ul><li>注意在这里我们就把它称为一个数，而不是一个“有一个数的向量”，这是不同的，就像 $ \bold{1(数) \neq (1)(矩阵))} $。因为一个数没有轴，即轴个数为0，轴长度也就没什么意义了，所以X的形状就是()，空的，啥也没有。可以把“()”理解成张量的标志。</li></ul><h2 id="向量——1D（阶）张量">向量——1D（阶）张量</h2><p>那么显然，向量（vector）是一串数字了，就比孤孤单单的一个数多了一个轴，而且这个轴也有长度了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的值：&quot;</span>, X)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的阶&quot;</span>, X.ndim)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的数据类型&quot;</span>, X.dtype)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的形状&quot;</span>, X.shape)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X的值： [1 2 3 4 5]</span><br><span class="line">X的阶 1</span><br><span class="line">X的数据类型 int64</span><br><span class="line">X的形状 (5,)</span><br></pre></td></tr></table></figure><ul><li>注意，分析以下这时候X的形状，()表示是个张量，里面有一个数字，表示它是1阶，数字为5，表示这一阶的维度（长度）为5。（有5个数呀）</li></ul><h3 id="向量的点积">向量的点积</h3><p>我们直到，两个向量可以做点积，中学都学过的：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>×</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><msub><mi>y</mi><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>2</mn></msub><msub><mi>y</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">(x_1, x_2) \times (y_1, y_2) = x_1y_1 + x_2y_2 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">Y = np.array([-<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,-<span class="number">2</span>])</span><br><span class="line">ans1 = np.dot(X, Y) <span class="comment"># 法一</span></span><br><span class="line">ans2 = X.dot(Y) <span class="comment"># 法二</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ans1)</span><br><span class="line"><span class="built_in">print</span>(ans2)</span><br><span class="line"><span class="comment"># ans1 = ans2 = -1</span></span><br></pre></td></tr></table></figure><ul><li>向量乘积遵循交换律: $ \vec A\vec B = \vec B\vec A $</li></ul><h2 id="矩阵——2D（阶）张量">矩阵——2D（阶）张量</h2><p><strong>矩阵</strong>（matrix）是一组一组向量的集合。矩阵就是2阶张量，它的形状为（<strong>样本，特征</strong>）。其表现形式为：</p><table><thead><tr><th></th><th>特征1</th><th>特征2</th><th>……</th><th>特征n</th></tr></thead><tbody><tr><td>样本1</td><td>数据(1,1)</td><td>数据(1,2)</td><td>……</td><td>数据(1,n)</td></tr><tr><td>样本2</td><td>数据(2,1)</td><td>……</td><td>……</td><td>……</td></tr><tr><td>……</td><td>……</td><td>……</td><td>……</td><td>……</td></tr><tr><td>样本n</td><td>数据(n,1)</td><td>……</td><td>……</td><td>数据(n,n)</td></tr></tbody></table><h3 id="矩阵的点积">矩阵的点积</h3><p>矩阵也可以进行点积，需满足矩阵乘法规则。这里具体就不说了。</p><h2 id="序列数据——3D（阶）张量">序列数据——3D（阶）张量</h2><p>如果说2D张量就像表格一样，那3D张量就很像多张表格叠在一起。在上一篇博客中，我们处理过MNIST数据集，实际上MNIST数据集是4D张量，因为图片会有一层深度信息，例如RGB就是三通道，深度为3，但是MNIST为灰度图像，只有1层深度，所以理论上可以把它当作3D张量处理。<strong>序列数据集</strong>才是真正的3D张量，其中最常见的是<strong>时间序列</strong>（time series）<br>其结构为：</p><ul><li>第一个轴：样本轴</li><li>第二个轴：时间步轴</li><li>第三个轴：特征轴</li></ul><blockquote><p>例如我记录杭州一个月的天气情况。特征包含（温度、湿度、风力），样本包含（1月1日，1月2日，……，1月30日），时间步长设定为：每30分钟测量一次上述的几个特征。就是这个意思，应该好理解。<br>也就是说，时许数据集的形状为：（样本，时戳，标签）</p></blockquote><h2 id="图像数据——4D张量">图像数据——4D张量</h2><p>刚才已经提到了，MNIST实际上就是4D张量。因为一张图片就有三个维度了，长、宽、通道数。</p><h2 id="视频数据——5D张量">视频数据——5D张量</h2><p>显而易见，视频又加了一维时间，但这个时间是用帧数来表示的，所以视频数据张量结构里比图像数据多了一维“帧”。</p><h2 id="通过索引和切片访问张量中的数据">通过索引和切片访问张量中的数据</h2><p>索引好理解，就像数组里面我们调用一个元素，会使用a[n]这样的方式。切片比较特殊，如果接触过matlab那么对这个用法会比较熟悉。下面看代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">array = np.arange(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(array)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;第四个元素：&#x27;</span>, array[<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最后一个元素&#x27;</span>, array[-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;从0到4切片&#x27;</span>, array[<span class="number">0</span>:<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;从0到12切片，步长为4&#x27;</span>, array[<span class="number">0</span>:<span class="number">12</span>:<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># [0 1 2 3 4 5 6 7 8 9]</span></span><br><span class="line"><span class="comment"># 第四个元素： 3</span></span><br><span class="line"><span class="comment"># 最后一个元素 9</span></span><br><span class="line"><span class="comment"># 从0到4切片 [0 1 2 3]</span></span><br><span class="line"><span class="comment"># 从0到12切片，步长为4 [0 4 8]</span></span><br><span class="line"></span><br><span class="line">array = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(array)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;第一列元素&#x27;</span>, array[:,<span class="number">0</span>:<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># 第一列元素 [[1]</span></span><br><span class="line"><span class="comment">#  [4]]</span></span><br></pre></td></tr></table></figure><p>再给出一个有点复杂的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(array[<span class="number">1</span>:<span class="number">2</span>],<span class="string">&#x27;它的形状是&#x27;</span>, array[<span class="number">1</span>:<span class="number">2</span>].shape)</span><br><span class="line"><span class="built_in">print</span>(array[<span class="number">1</span>:<span class="number">2</span>][<span class="number">0</span>],<span class="string">&#x27;它的形状是&#x27;</span>, array[<span class="number">1</span>:<span class="number">2</span>][<span class="number">0</span>].shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [[4 5 6]] 它的形状是 (1, 3)</span></span><br><span class="line"><span class="comment"># [4 5 6] 它的形状是 (3,)</span></span><br></pre></td></tr></table></figure><ul><li>看起来同样是4,5,6，但第一条是两个括号括起来的，里面的[4,5,6]实际上是一个整体，而第二条就是三个元素。两者阶数不相同。<strong>张量是机器学习的数据结构，其形状是数据处理的关键！</strong></li></ul><h3 id="张量的整体操作和逐元素运算">张量的整体操作和逐元素运算</h3><p>张量的加减乘除乘方都可以整体进行或者逐元素进行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">array = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(array)</span><br><span class="line">array += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(array)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(array.shape[<span class="number">0</span>]):</span><br><span class="line">  array[i] += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(array)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [1 2 3]</span></span><br><span class="line"><span class="comment"># [2 3 4]</span></span><br><span class="line"><span class="comment"># [3 4 5]</span></span><br></pre></td></tr></table></figure><h3 id="张量的变形和转置">张量的变形和转置</h3><blockquote><p>使用reshape方法进行变形和转置，T方法进行转置</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">array = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(array,<span class="string">&#x27;它的形状是&#x27;</span>, array.shape)</span><br><span class="line"><span class="built_in">print</span>(array.reshape(<span class="number">1</span>,<span class="number">6</span>),<span class="string">&#x27;它的形状是&#x27;</span>, array.reshape(<span class="number">1</span>,<span class="number">6</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(array.T,<span class="string">&#x27;它的形状是&#x27;</span>,array.T.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]] 它的形状是 (2, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [[1 2 3 4 5 6]] 它的形状是 (1, 6)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [[1 4]</span></span><br><span class="line"><span class="comment">#  [2 5]</span></span><br><span class="line"><span class="comment">#  [3 6]] 它的形状是 (3, 2)</span></span><br></pre></td></tr></table></figure><ul><li>注意，调用reshape方法和T方法时，原array并没有变，要通过赋值才能改变原array的值</li><li>注意，调用rashape的时候一定要保证数据个数乘起来时一样的，不然会报错。</li></ul><h2 id="广播">广播</h2><p>Python的广播功能是numpy对形状不完全相同的数组间进行计算的方式。给出示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">array_1 = np.array([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>],[<span class="number">20</span>,<span class="number">20</span>,<span class="number">20</span>],[<span class="number">30</span>,<span class="number">30</span>,<span class="number">30</span>]])</span><br><span class="line">array_2 = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]])</span><br><span class="line">array_3 = np.array([[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line">list_1=[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;array_2的形状：&quot;</span>, array_2.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;array_3的形状：&quot;</span>, array_3.shape)</span><br><span class="line">array_4 = array_2.reshape(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;array_4的形状：&quot;</span>, array_4.shape)</span><br><span class="line">array_5 = np.array([<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;array_5的形状：&quot;</span>, array_5.shape)</span><br><span class="line">array_6 = array_5.reshape(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;array_6的形状：&quot;</span>, array_6.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;1+2的结果:&quot;</span>, array_1+array_2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;1+3的结果:&quot;</span>, array_1+array_3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;1+4的结果:&quot;</span>, array_1+array_4)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;1+5的结果:&quot;</span>, array_1+array_5)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;1+6的结果:&quot;</span>, array_1+array_6)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;1+1的结果:&quot;</span>, array_1+list_1)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">array_2的形状： (1, 3)</span><br><span class="line">array_3的形状： (4, 1)</span><br><span class="line">array_4的形状： (3,)</span><br><span class="line">array_5的形状： (1,)</span><br><span class="line">array_6的形状： (1, 1)</span><br><span class="line">1+2的结果: [[ 0  1  2]</span><br><span class="line"> [10 11 12]</span><br><span class="line"> [20 21 22]</span><br><span class="line"> [30 31 32]]</span><br><span class="line">1+3的结果: [[ 0  0  0]</span><br><span class="line"> [11 11 11]</span><br><span class="line"> [22 22 22]</span><br><span class="line"> [33 33 33]]</span><br><span class="line">1+4的结果: [[ 0  1  2]</span><br><span class="line"> [10 11 12]</span><br><span class="line"> [20 21 22]</span><br><span class="line"> [30 31 32]]</span><br><span class="line">1+5的结果: [[ 1  1  1]</span><br><span class="line"> [11 11 11]</span><br><span class="line"> [21 21 21]</span><br><span class="line"> [31 31 31]]</span><br><span class="line">1+6的结果: [[ 1  1  1]</span><br><span class="line"> [11 11 11]</span><br><span class="line"> [21 21 21]</span><br><span class="line"> [31 31 31]]</span><br><span class="line">1+1的结果: [[ 0  1  2]</span><br><span class="line"> [10 11 12]</span><br><span class="line"> [20 21 22]</span><br><span class="line"> [30 31 32]]</span><br></pre></td></tr></table></figure><h1>机器学习的几何意义</h1><h2 id="机器学习的向量空间">机器学习的向量空间</h2><p>张量，可以被解释为某种几何空间内点的坐标。这样，机器学习中特征向量就形成了特征空间。考虑一个二维向量：A=(0.5, 1).我们可以把它想成一个二维坐标系的一个点。而更高维度的张量实际可以把它想象成更高维度的点。<br>推而广之：**机器学习模型是在更高维度的几何空间中对特征向量进行操作、变形，计算其间的距离，并寻找从特征向量到标签之间的函数拟合——这就是从几何角度所阐述的机器学习本质。</p><blockquote><p>几种常见的的机器学习模型都可以通过特征空间进行几何描述。</p></blockquote><ul><li>回归模型：需要找到最合适的方式去拟合样本空间</li><li>分类模型：找到一个分割超平面将特征空间分成两个类</li><li>聚类模型：通过对特征空间中的特征实施某种相似性的度量，将相近的特征聚在一起。</li></ul><h2 id="深度学习和数据流形">深度学习和数据流形</h2><p>深度学习的过程实际上就是数据提纯的过程。主要因为特征维度过高，导致特征空间十分复杂，进而导致机器学习建模难度过大。有一种是通过<strong>流形</strong>（manifold）学习将高维特征空间中的样本分布群“平铺”至一个低维空间，同时能保存原高维空间中样本点之间的局部位置的相关信息。<br>在传统的机器学习中，流形学习主要用于特征提取和数据降维，特征提取使特征变得更加友好，降维是因为高维数据通常有冗余。<br>而在深度学习出现之后，有一种说法认为神经网络能够自动自发地将复杂的特征数据流形展开，从而减少了特征提取的需要。<br>就像一张写了很多信息的纸但是揉皱了，我们只需要把它展开读取信息就行。<br>因此，现代的<strong>深度神经网络（DNN）通过参数学习，展开了高维数据的流形——这可以说是深度学习的几何意义。</strong></p><h1>课后练习</h1><blockquote><p>一 变量(x, y)的集合{(-5, 1),(3, -3),(4, 0),(3, 0),(-4, 3)}是否满足函数的定义？为什么？</p></blockquote><p>答：满足。因为x与y一一对应，且没有同一个x值对应两个y。</p><blockquote><p>二 画出线性函数y=2x+1的函数图像，并在图中标出其斜率和y轴上的截距</p></blockquote><p>答：斜率2，截距1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = np.arange(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">0.01</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">1</span></span><br><span class="line">plt.plot(x, y)</span><br></pre></td></tr></table></figure><blockquote><p>三 在上一节中，我们曾使用语句from keras.datasets import boston_house导入了波士顿房价数据集。请同学们输出这个房价数据集对应的数据张量，并说出这个张量的形状。</p></blockquote><blockquote><p>四 对波士顿房价数据集的数据张量进行切片操作，输出其中第101~200个数据样本。</p></blockquote><p>三四答：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> boston_housing</span><br><span class="line">(X_train_data, y_train_label), (X_test_data, y_test_label) = boston_housing.load_data()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集张量形状：&quot;</span>, X_train_data.shape)</span><br><span class="line"><span class="built_in">slice</span> = X_train_data[<span class="number">100</span>:<span class="number">200</span>,:]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第101~200个数据样本为：&quot;</span>, <span class="built_in">slice</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">数据集张量形状： (<span class="number">404</span>, <span class="number">13</span>)</span><br><span class="line">第<span class="number">101</span>~<span class="number">200</span>个数据样本为： [[<span class="number">6.12900e-02</span> <span class="number">2.00000e+01</span> <span class="number">3.33000e+00</span> ... <span class="number">1.49000e+01</span> <span class="number">3.77070e+02</span></span><br><span class="line">  <span class="number">3.01000e+00</span>]</span><br><span class="line"> [<span class="number">5.75290e-01</span> <span class="number">0.00000e+00</span> <span class="number">6.20000e+00</span> ... <span class="number">1.74000e+01</span> <span class="number">3.85910e+02</span></span><br><span class="line">  <span class="number">2.47000e+00</span>]</span><br><span class="line"> [<span class="number">4.75470e-01</span> <span class="number">0.00000e+00</span> <span class="number">9.90000e+00</span> ... <span class="number">1.84000e+01</span> <span class="number">3.96230e+02</span></span><br><span class="line">  <span class="number">1.27300e+01</span>]</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">2.36482e+01</span> <span class="number">0.00000e+00</span> <span class="number">1.81000e+01</span> ... <span class="number">2.02000e+01</span> <span class="number">3.96900e+02</span></span><br><span class="line">  <span class="number">2.36900e+01</span>]</span><br><span class="line"> [<span class="number">4.98100e-02</span> <span class="number">2.10000e+01</span> <span class="number">5.64000e+00</span> ... <span class="number">1.68000e+01</span> <span class="number">3.96900e+02</span></span><br><span class="line">  <span class="number">8.43000e+00</span>]</span><br><span class="line"> [<span class="number">6.41700e-02</span> <span class="number">0.00000e+00</span> <span class="number">5.96000e+00</span> ... <span class="number">1.92000e+01</span> <span class="number">3.96900e+02</span></span><br><span class="line">  <span class="number">9.68000e+00</span>]]</span><br></pre></td></tr></table></figure><blockquote><p>用python生成形状如下的两个张量，确定其阶的个数，并进行点积操作，最后输出结果。<br>A = [1, 2, 3, 4, 5]<br>B = [[5],[4],[3],[2],[1]]</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">B = np.array([[<span class="number">5</span>],[<span class="number">4</span>],[<span class="number">3</span>],[<span class="number">2</span>],[<span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A的阶数：&quot;</span>, A.ndim)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;B的阶数：&quot;</span>, B.ndim)</span><br><span class="line"><span class="comment"># 点积操作</span></span><br><span class="line">ans = np.dot(A, B)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A与B点积结果为：&quot;</span>, ans)</span><br><span class="line"></span><br><span class="line"><span class="comment"># A的阶数： 1</span></span><br><span class="line"><span class="comment"># B的阶数： 2</span></span><br><span class="line"><span class="comment"># A与B点积结果为： [35]</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;基础数学知识&lt;/h1&gt;
&lt;h1&gt;机器学习中的函数&lt;/h1&gt;
&lt;p&gt;**机器学习基本上等价于寻找函数的过程。**机器学习的目的是进行预测、判断，实现某种功能。通过学习训练集中的数据，计算机得到一个从x到y的拟合结果，也就是函数。例如：&lt;/p&gt;
&lt;p&gt;&lt;span class=</summary>
      
    
    
    
    <category term="人工智能学习路线" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    <category term="从零开始机器学习" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="人工智能" scheme="https://yigexiaogai.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="机器学习，毕设相关" scheme="https://yigexiaogai.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E6%AF%95%E8%AE%BE%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战（一）</title>
    <link href="https://yigexiaogai.github.io/2022/04/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://yigexiaogai.github.io/2022/04/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2022-04-02T12:50:39.000Z</published>
    <updated>2022-06-04T14:01:17.792Z</updated>
    
    <content type="html"><![CDATA[<h1>前言</h1><p>作者大四在读，恰逢毕设时间，毕设选题将会以GAN技术（生成式对抗网络技术）为核心展开工作。然而作者在这之前，并没有任何与智能图片生成——也就是“有关于深度学习的图像生成技术”打过交道。作者首先接触到了<a href="https://www.bilibili.com/video/BV1Up411R7Lk?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click">B站台湾大学李宏毅教授的GAN课程</a>，奈何缺乏基本的机器学习、神经网络、深度学习等前要知识，听到第二节课就已经云里雾里了（很烦~），最后我决定从头开始，毕竟磨刀不误砍柴工，打好基础才能实现上层目标。根据人工智能领域的分支关系图，我决定先从<strong>机器学习</strong>开始。</p><h1>学习路线（机器学习）</h1><h2 id="网课（理论）">网课（理论）</h2><p>理论部分我采用网课形式。在阿里巴巴旗下的子平台<a href="https://edu.aliyun.com/developer">阿里云全球培训中心：开发者课堂</a>中有着众多领域的学习路线，其中包含<a href="https://edu.aliyun.com/roadmap/ai">人工智能学习路线</a>，阶段一即是机器学习入门。</p><img src="/2022/04/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.png" class><p>网课老师语速和节奏较慢，关键是在一些概念性的问题上讲解通俗易懂。不过到中期算法讲解之后，由于学习机器学习数学知识非常必要，内容不可能轻易让人理解。（学习机器学习前，数学中的<strong>线性代数</strong>和<strong>概率统计</strong>是非常重要的内容）</p><h3 id="书籍（实战）">书籍（实战）</h3><p>其实在书籍方面，最有口皆碑的就是周志华教授的《机器学习》一书，也被称作“西瓜书”，其内容非常经典而且适合学习。</p><img src="/2022/04/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/%E8%A5%BF%E7%93%9C%E4%B9%A6.jpg" class><p>不过在作者学校的图书馆此书已经被借空，而且此书的学习实际上要求学生已经掌握了一些人工智能的基础知识，因此最后，我选择了黄佳编著的《零基础学人工智能》一书，里面包含若干个机器学习项目实例，并附有学习资源和代码。这些资源在书籍扉页可以找到(<a href="https://www.epubit.com/bookDetails?id=UB7245bf2ca7715">https://www.epubit.com/bookDetails?id=UB7245bf2ca7715</a>)。</p><h3 id="语言及框架">语言及框架</h3><p>python。这个其实也不用多说了，众所周知搞人工智能最好的编程语言。不过关于人工智能框架需要说一点，在学习过程中，机器学习主要接触的是scikit-learn库，深度学习主流的框架有三个：keras、tensorflow、pytorch。书籍中所使用的皆为keras,而我本人的项目也会涉及到tensorflow。</p><h3 id="平台及环境">平台及环境</h3><p>我使用的平台有两个，分为本地环境和线上环境。本地环境使用anaconda创建一个python环境，并安装jupyter notebook库便于写代码。但是由于各种各样的原因，在研究过程中我还需要用到一个线上平台，及google colab(需要科学上网)，colab 支持用户在google的服务器上使用jupyter notebook,而且最<strong>关键</strong>的两点是：<strong>1.你不需要特地配置keras、tensorflow等环境（本地配置的时候可能会存在各种问题，例如库版本、库间对应版本……），它已经帮你配好了，只需要开写代码就行！2.一般做机器学习项目需要电脑硬件比较好，但是colab支持用户设备不需要GPU硬件——就是你电脑没显卡也没事儿，用的是服务器的GPU,运行代码速度照样嘎嘎快（没有那么夸张）</strong></p><p>书中推荐的平台还有一个叫做<a href="https://www.kaggle.com/">kaggle</a>，该平台兴办机器学习竞赛，而且同样向平台用户提供GPU，但是每周使用有一定限制。</p><h1>实例一：加州房价预测</h1><p>该实例来源自参考书籍中，作为读者第一个接触到的实例，不需要看懂每行代码，只需要观察机器学习项目的代码结构以及思路即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># 导入Pandas，用于数据读取和处理</span></span><br><span class="line"><span class="comment"># 读取房价数据，示例代码中的文件地址为internet链接，读者也可以下载该文件到本机进行读取(见注释代码行)</span></span><br><span class="line">df_housing = pd.read_csv(<span class="string">&quot;https://raw.githubusercontent.com/huangjia2019/house/master/house.csv&quot;</span>)</span><br><span class="line"><span class="comment"># df_housing = pd.read_csv(&quot;./house.csv&quot;) # 与代码文件放在同一目录下</span></span><br><span class="line">df_housing.head() <span class="comment"># 显示加州房价数据</span></span><br></pre></td></tr></table></figure><ul><li>使用DataFrame数据结构的head方法显示数据集中的部分信息</li><li>在jupyter notebook中按ctrl+enter运行一个块儿（cell）中的代码</li></ul><img src="/2022/04/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/1.png" class><p>该数据集记录了加州各地区的房价统计信息。看到最后一列就是房价的统计结果，很直观可以看出，影响这个结果的就是前几列的因素：经度、纬度、房屋平均年龄、房屋数量、家庭收入中位数。</p><p>因此我们可以说，前面这些影响因素就是数据集的<strong>特征</strong>，最后这列我们要的结果就是<strong>标签</strong>。我们的目标呼之欲出，我们要从这个数据集中归纳（拟合）出一个函数<strong>f(经度，纬度，……，家庭收入中位数)-&gt;房价</strong>。有了这个函数之后，我们去加州随便找一个地方的房子，都能算出这个房子的价格了，然后你就能算算你买不买得起了（笑）。</p><p>现在我们就来做找函数这件事。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = df_housing.drop(<span class="string">&quot;median_house_value&quot;</span>, axis=<span class="number">1</span>) <span class="comment"># 构建特征集</span></span><br><span class="line">y = df_housing.median_house_value <span class="comment"># 构建标签集y</span></span><br></pre></td></tr></table></figure><ul><li>在机器学习领域特征集一般用大写X表示，标签集一般用小写y表示，可能是习惯？</li><li>使用drop方法（该方法来自于pandas库中的dataframe数据结构）把最后一列字段去掉形成特征集</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="comment"># 导入sklearn工具库</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>) <span class="comment"># 以80%/20%的比例拆分数据集</span></span><br></pre></td></tr></table></figure><ul><li>把数据集拆分成两块，80%的数据用于产生我们需要的函数f，另外20%的数据用于带入数据比较结果，看看这个函数找的对不对。在网课内容中，我们知道，不用整个数据集找函数是怕找到的函数<strong>过拟合</strong>——即看起来f效果不错（对该数据集来说），但假设你用额外的新数据带入,f的效果可能会不好，但这在你带入新数据前都无法知道。</li></ul><p>接下来开始训练机器，首先我们需要找一个适合该问题的模型。这里选择LinearRegression(线性回归)方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(X_train, y_train) <span class="comment"># 根据训练集数据，训练机器，拟合函数</span></span><br></pre></td></tr></table></figure><ul><li>fit方法即可以训练机器，拟合函数，函数的具体形式已经在model中了，我们没有直接看到。下面进行预测（这里参与预测的数据就是刚刚分离出来的20%的数据咯！）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y_pred=model.predict(X_test) <span class="comment"># 预测验证集的y值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;房价的真值（测试集）&quot;</span>, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测的房价（测试集）&quot;</span>, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;给预测评分：&quot;</span>, model.score(X_test, y_test)) <span class="comment"># 预测评估结果</span></span><br></pre></td></tr></table></figure><img src="/2022/04/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/2.png" class title="输出结果"><ul><li>predict方法应该是用上了刚才模型训练得到的函数f，可得到y_pred。</li><li>score方法是一个机器学习模型的评估指标，给出了预测值的方差与总体方差之间的差异。根据score中的参数，个人觉得score里面应该是先用了predict方法求取了y_pred，然后进行比较。公式如图：</li></ul><img src="/2022/04/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/%E5%85%AC%E5%BC%8F.png" class><p>接下来我们使用matplotlib库把函数可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 用散点图显示家庭收入中位数和房价中位数的分布</span></span><br><span class="line">plt.scatter(X_test.median_income, y_test, color=<span class="string">&#x27;brown&#x27;</span>)</span><br><span class="line"><span class="comment"># 画出回归函数（从特征到预测标签）</span></span><br><span class="line">plt.plot(X_test.median_income, y_pred, color=<span class="string">&#x27;green&#x27;</span>,linewidth=<span class="number">1</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Median Income&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Median House Value&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/2022/04/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/3.png" class><ul><li>注意图里看上去有很多线，但实际上它是一条线，因为plot只是把点与点连起来画出一条线，也就是说代码展示了median_income和y_pred的大概函数关系，大概是线性增长的。</li></ul><h1>基本机器学习术语</h1><table><thead><tr><th>术语</th><th>定义</th><th>数学描述</th><th>示例</th></tr></thead><tbody><tr><td>数据集</td><td>数据的集合</td><td>{(X~1~,y~1~), …, (X~n~,y~n~)}</td><td>1000个北京市房屋的面积、楼层、位置、朝向，以及部分房价信息的数据集</td></tr><tr><td>样本</td><td>数据集中的一条具体记录</td><td>(X~1~,y~1~)</td><td>一个房屋的数据记录</td></tr><tr><td>特征</td><td>用于描述数据的输入变量</td><td>{X~1~,X~2~, …, X~N~}也是一个向量</td><td>面积(X~1~)、楼层(X~2~)、位置(X~3~)、朝向(X~4~)</td></tr><tr><td>标签</td><td>要预测的真实事物或结果，也称为目标</td><td>y</td><td>房价</td></tr><tr><td>有标签样本</td><td>有特征、标签、用于训练模型</td><td>(X,y)</td><td>800个北京市房屋的面积、楼层、位置、朝向、以及房价信息</td></tr><tr><td>无标签样本</td><td>有特征，无标签</td><td>(X,?)</td><td>200个北京市房屋的面积、楼层、位置、朝向，但是无房价信息</td></tr><tr><td>模型</td><td>将样本的特征映射到预测标签</td><td>f(x),其实就是函数</td><td>通过面积、楼层、位置、朝向这些信息来确定房价的函数</td></tr><tr><td>模型中的参数</td><td>模型中的参数确定了机器学习的具体模型</td><td>f(x)函数的参数</td><td>如f(x)=5x+6中的5和6</td></tr><tr><td>模型的映射结果</td><td>通过模型映射出无标签样本的标签</td><td>y’</td><td>200个被预测出来的房价</td></tr><tr><td>机器学习</td><td>通过学习样本数据，发现规律，得到模型的参数，从而得到能预测目标的模型</td><td>确定f(x)和其参数的过程</td><td>确定房价预测函数和具体参数的过程</td></tr></tbody></table><blockquote><p>特征向量中有<strong>几</strong>个特征，就说明这个特征是<strong>几</strong>维特征<br>y^'^有时候也被成为y-hat，写作y上面带一个^</p></blockquote><h1>机器学习项目实战架构与MNIST数据集</h1><p>机器学习项目的实际过程主要分为以下5个环节：</p><ul><li>（1）问题定义。</li><li>（2）数据的收集和预处理</li><li>（3）模型（算法）的选择</li><li>（4）选择机器学习模型</li><li>（5）超参数调试和性能优化</li></ul><h2 id="环节一：问题定义">环节一：问题定义</h2><p>问题定义简单来说，就是你想要解决什么问题，它是否可以用机器学习的方法解决。<br>MNIST数据集相当于机器学习领域的hello world，相当经典，其中包含60000张训练图像和10000张测试图像，尺寸都为28px*28px，如下图所示。</p><img src="/2022/04/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/4.jpg" class><p>此处要解决的问题是：将手写数字灰度图像分类为0，1，2，3，4，5，6，7，8，9</p><h2 id="环节二：数据收集和预处理">环节二：数据收集和预处理</h2><p>该环节主要包括以下内容：</p><ul><li>原始数据的准备</li><li>数据的预处理</li><li>特征工程和特征提取</li></ul><h3 id="原始数据的准备">原始数据的准备</h3><p>可以是自有数据（如公司的客户信息）、使用爬虫爬取的数据、开源网站下载的数据、来自youtube和维基百科的数据……</p><h3 id="数据的预处理">数据的预处理</h3><ul><li><strong>可视化（visualization）</strong>:使用excel和各种数据分析工具（如matplotlib）从各种角度看数据。</li><li><strong>数据向量化（data vectorization）</strong>：把原始数据格式化，使其变得机器可以读取，例如把图片转换成数字矩阵，把文字转换成one-hot编码（<a href="https://www.cnblogs.com/shuaishuaidefeizhu/p/11269257.html">one-hot编码概念</a>）。</li><li>处理<strong>坏数据</strong>和<strong>缺失值</strong>：常常一组数据不是所有都能用。</li><li><strong>特征缩放（feature scaling）</strong>:常用方法有<strong>标准化（standardization）<strong>和</strong>规范化（normalization）</strong></li></ul><h3 id="特征工程和特征提取">特征工程和特征提取</h3><p>广义上仍属于数据预处理。特征提取主要需要思考以下问题：</p><blockquote><p>（1）如何选择最有用的特征给机器进行学习？<br>（2）如何把现有的特征进行转化、强化、组合，创建出来新的、更好的特征？<br>比如，对于图像数据，可以通过计算直方图来统计图像中像素强度的分布，得到描述图像颜色的特征。</p></blockquote><h3 id="实例：载入MNIST数据集">实例：载入MNIST数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist <span class="comment"># 从keras中导入MNIST数据集</span></span><br><span class="line"><span class="comment"># 读入训练集和测试集</span></span><br><span class="line">(X_train_image, y_train_label), (X_test_image, y_test_label) = mnist.load_data()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集张量形状：&quot;</span>, X_train_image.shape) <span class="comment"># 用shape方法显示张量的形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第一个数据样本：\n&quot;</span>, X_train_image[<span class="number">0</span>]) <span class="comment"># 注意python的索引是从0开始的</span></span><br></pre></td></tr></table></figure><blockquote><p>X_train_image:训练集特征——图片<br>y_train_label:训练集标签——数字<br>X_test_image:测试集特征——图片<br>y_test_label:测试集标签——数字</p></blockquote><ul><li>shape方法显示X_train_image张量的形状：60000表示60000张，28、28表示尺寸</li></ul><p>数据集在输入机器学习模型之前还要做一些数据格式转换工作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.utils <span class="keyword">import</span> to_categorical <span class="comment"># 导入keras.utils工具库的类别转换工作</span></span><br><span class="line"><span class="comment">#在kaggle可以写成from keras.utils import to_categorical，在colab需要加“tensorflow.”，似乎是tensorflow2.0版本的问题</span></span><br><span class="line">X_train = X_train_image.reshape(<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>) <span class="comment"># 给标签增加一个维度</span></span><br><span class="line">X_test = X_test_image.reshape(<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line">y_train = to_categorical(y_train_label, <span class="number">10</span>) <span class="comment"># 特征转换为one-hot编码，10代表分成10类</span></span><br><span class="line">y_test = to_categorical(y_test_label, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练集张量形状：&quot;</span>, X_train.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第一个数据标签：&quot;</span>, y_train[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><ul><li><a href="https://blog.csdn.net/moyu123456789/article/details/83444140">to_categorical()方法介绍</a></li><li>要增加一个维度的原因：keras要求图像数据集导入卷积神经网络时为4阶张量，最后一阶代表颜色深度，灰度图像只有一个颜色通道，所以设置值为1（如果时rgb图像那就是3咯）</li><li>标签[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]代表1，[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]代表2，以此类推。</li></ul><h2 id="环节三：选择机器学习模型">环节三：选择机器学习模型</h2><p>常见的模型框架有以下几种：</p><ul><li>线性模型（线性回归、逻辑回归）</li><li>非线性模型（支持向量机、KNN）</li><li>基于树和集成的模型（决策树、随机森林、梯度提升树）</li><li><strong>神经网络</strong>（ANN、CNN、长短期记忆网络）</li></ul><p>在这里我们选择CNN（卷积神经网络）处理MNIST。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models <span class="comment"># 导入Keras模型，以及各种神经网络的层</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Dropout, Flatten, Conv2D, MaxPooling2D</span><br><span class="line">model = models.Sequential() <span class="comment"># 用序贯方式建立模型</span></span><br><span class="line">model.add(Conv2D(<span class="number">32</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, <span class="comment">#添加Conv2D层</span></span><br><span class="line">                 input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))) <span class="comment"># 指定输入数据样本张量的类型</span></span><br><span class="line">model.add(MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>))) <span class="comment"># 添加MaxPooling2D层</span></span><br><span class="line">model.add(Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加Conv2D层</span></span><br><span class="line">model.add(MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>))) <span class="comment"># 添加MaxPooling2D层</span></span><br><span class="line">model.add(Dropout(<span class="number">0.25</span>)) <span class="comment"># 添加Dropout层</span></span><br><span class="line">model.add(Flatten()) <span class="comment"># 展平</span></span><br><span class="line">model.add(Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加全连接层</span></span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>)) <span class="comment"># 添加Dropout层</span></span><br><span class="line">model.add(Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)) <span class="comment"># Softmax分类激活，输出10维分类码</span></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;rmsprop&#x27;</span>, <span class="comment"># 指定优化器</span></span><br><span class="line">              loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, <span class="comment"># 指定损失函数</span></span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>]) <span class="comment"># 指定验证过程中的评估指标</span></span><br></pre></td></tr></table></figure><ul><li>这一段我也看不懂，希望以后学了就会懂吧5555</li></ul><h2 id="环节四：训练机器，确定参数">环节四：训练机器，确定参数</h2><p>在这里我们需要确定模型<strong>内部参数</strong>，简单来说就是f(x)的形式呗。例如f(x)=2x+1，这里的2叫做<strong>权重</strong>，1叫做<strong>偏置</strong>。以后还会学到<strong>超参数</strong>一概念。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X_train, y_train, <span class="comment"># 指定训练特征集和训练标签集</span></span><br><span class="line">          validation_split=<span class="number">0.3</span>, <span class="comment"># 部分训练集数据拆分成验证集</span></span><br><span class="line">          epochs=<span class="number">5</span>, <span class="comment"># 训练轮次为5轮</span></span><br><span class="line">          batch_size=<span class="number">128</span>) <span class="comment"># 以128为批量进行训练</span></span><br></pre></td></tr></table></figure><ul><li>以上5轮训练中，准确率会逐渐提高。</li><li>这里的5轮就是一种超参数，是可以人为定义的且与f无关，但最后仍有可能影响训练结果。</li></ul><h2 id="环节五：超参数调试和性能优化">环节五：超参数调试和性能优化</h2><p>机器学习重在评估，通过评估才能知道当前模型的效率。<br>两个重要评估点：</p><ul><li>损失函数</li><li>验证</li></ul><p>本例中的损失函数已经包含在了fit方法中了。验证则可以使用以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">score = model.evaluate(X_test, y_test) <span class="comment"># 在验证集上进行模型评估</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试集预测准确率：&quot;</span>, score[<span class="number">1</span>]) <span class="comment"># 输出测试集上的预测准确率</span></span><br></pre></td></tr></table></figure><blockquote><p>当然，人们看到一个准确率常常没有直观的体验，虽然说这个时候机器学习项目已经结束了。那么我们单独拿一个预测结果看看对不对。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pred = model.predict(X_test[<span class="number">3</span>].reshape(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)) <span class="comment"># 预测测试集第4个数据</span></span><br><span class="line"><span class="built_in">print</span>(pred[<span class="number">0</span>], <span class="string">&quot;转换格式得到：&quot;</span>, pred.argmax()) <span class="comment"># 把one-hot编码转换为数字</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.imshow(X_test[<span class="number">3</span>].reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=<span class="string">&#x27;Greys&#x27;</span>) <span class="comment"># 输出图片</span></span><br></pre></td></tr></table></figure><ul><li>argmax方法就是输出数组里面最大元素的索引。例如[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]就是输出2（第三个咯）。<br>输出结果如下：</li></ul><img src="/2022/04/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/5.png" class><p>发现结果正确，都是0。</p><h1>课后练习和答案（非标答）</h1><blockquote><p>一 列举出机器学习的类型，并说明分类的标准</p></blockquote><p>答：监督学习、无监督学习、半监督学习，分类标准是已有的数据集是否有标签。</p><blockquote><p>二 解释机器学习术语：什么是特征，什么是标签，什么是机器学习模型</p></blockquote><p>答：特征：用于描述模型的输入数据。标签：数据的输出结果。机器学习模型：将样本特征映射到标签的一种方法。</p><blockquote><p>自己导入keras的波士顿房价数据集（boston_housing），并判断哪些是标签字段。<br>参考两个机器学习代码，使用LinearRegression线性回归算法对波士顿房价数据集进行建模。</p></blockquote><p>两者一起答：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> boston_housing</span><br><span class="line">(X_train_data, y_train_label), (X_test_data, y_test_label) = boston_housing.load_data()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集张量形状：&quot;</span>, X_train_data.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第一个样本数据：\n&quot;</span>, X_train_data[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第一个样本标签：\n&quot;</span>, y_train_label[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(X_train_data, y_train_label)</span><br><span class="line"></span><br><span class="line">y_pred = model.predict(X_test_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;房价的真值(测试集)&#x27;</span>,y_test_label)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测的房价(测试集)&#x27;</span>,y_pred)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;给模型评分：&quot;</span>, model.score(X_test_data, y_test_label))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;前言&lt;/h1&gt;
&lt;p&gt;作者大四在读，恰逢毕设时间，毕设选题将会以GAN技术（生成式对抗网络技术）为核心展开工作。然而作者在这之前，并没有任何与智能图片生成——也就是“有关于深度学习的图像生成技术”打过交道。作者首先接触到了&lt;a href=&quot;https://www.bili</summary>
      
    
    
    
    <category term="人工智能学习路线" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    <category term="从零开始机器学习" scheme="https://yigexiaogai.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="人工智能" scheme="https://yigexiaogai.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="机器学习，毕设相关" scheme="https://yigexiaogai.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E6%AF%95%E8%AE%BE%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>我的第一篇博客</title>
    <link href="https://yigexiaogai.github.io/2022/03/28/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
    <id>https://yigexiaogai.github.io/2022/03/28/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</id>
    <published>2022-03-28T13:56:37.000Z</published>
    <updated>2022-06-04T14:12:52.951Z</updated>
    
    <content type="html"><![CDATA[<h1>前言</h1><p>嗨，屏幕前的你！欢迎来到Gai的第一篇博客！（这句话同样献给我自己）<br>我曾经试过使用各种方式来描写这段开头，但最终都放弃了，因为我实在有太多的话想说而不知从何开始。总之，当你看到这一篇博客的时候，就说明我成功地将我的生活、我的世界分享给了你。<br>So, come on and let’s be friends!</p><img src="/2022/03/28/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/2022-03-29-19-05-39.png" class title="This is an example image"><h1>诞生</h1><p>很显然，在这样一个信息化的世界中，个人博客已经成为了一个技术宅的标配。对于极客们来说，无论是信息输出还是信息输入，博客都是一个良好的渠道——当然，大部分人可能仅限于“输入”的部分吧。</p><p>CSDN、博客园、知乎、公众号，甚至是B站，它们都可以成为一种“博客”，人人都能在平台的账号中创作+管理+共享内容。可对我来说，这大概还不够cool~事实上我也有想过啦，和有的同学一样在CSDN上写个人博客，毕竟我是个啥都不懂的小白，个人主页什么的，一定超级麻烦的吧！（现在我依然这么想！）</p><p>应该是在大二的时候，我在一门技术相关的课程上遇到了一个无法解决的问题（具体是什么课程已经忘记了），于是自然而然我去CSDN上寻找答案，巧的是，我看到了一个似乎是来自同一所学校的同学写的博客，顺藤摸瓜我找到了他的个人主页。这个主页经典而又美观，在彩色的背景上整齐摆放着各种技术内容，右下角有一虚拟角色可以互动，背景的两侧有能与鼠标交互的几何形状。在一番求证之后，我确信，博主正正好好就是坐在我旁边的同学。他是个转专业来的学生，平时默默无闻，因此我和他不熟。按捺不住内心的激动，我偷瞄了一眼他的电脑屏幕，他还是不动声色地写着代码，从画面上可以看出是某种小游戏。我不好意思与他说明，只是在内心中觉得：wow，这太酷了！</p><p><em>我身边大佬的个人博客（我偷偷贴了）</em>：<a href="https://jinjis.cn/">大佬的博客</a></p><p>博客最大的用处我想应该有两个：分享和整理。哼，别看我平时乱糟糟的，我其实很爱整理。我会把喜欢的电影、漫画资源和用过的软件安装包放在移动硬盘里；我会把我的学习笔记分门别类重新整理在Goodnotes中；我会把剪辑视频用到的素材都好好地整理备份；从两年前开始，我会把所有旅行相关的事物贴在一本精致的小本本里，包括车票、门票等等，接着在旁边批注或者写下心得，有时还会画下图案……可唯独，唯独我的programs只能躺在“用户”的“文档”中（也可能是repo)。所以当看到大佬的个人博客时，我真的狠狠心动了。</p><p>于是我下定决心克服万难，创造一个属于我自己的小天地，在这里，我不仅可以将我以前整理好的东西迁入，更可以添加新的事物，比如我的项目、代码，哦对，我也许可以开始写日记！（正经人谁写日记啊）可以加一个健身打卡哈哈，又或者是有感而发的漫评、影评、书评……</p><h1>过程</h1><p>在很早之前我就开始准备筹划个人博客了，但可供选择的框架和平台很多，要学习的事物也很多。最终我选择hexo+github部署的方法实现静态博客，因为这样既不用花钱买服务器，也（可以）不用花钱买域名，相当方便。在这个过程中，我逐渐接触了nodejs、npm、hexo、命令行语言、markdown……各种各样的新鲜东西。总之，进行这样一个全新的项目更让我感觉到自己的无知与渺小。</p><p>在这里我贴出我在搭建个人博客时用到的各种资源：<br>参考视频：<a href="https://www.bilibili.com/video/BV1Yb411a7ty?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click">手把手教你从0开始搭建自己的个人博客</a>、<a href="https://www.bilibili.com/video/BV1hJ411X75X?spm_id_from=333.337.search-card.all.click">15分钟快速上手Markdown教程</a><br>参考文档：<a href="https://hexo.io/zh-cn/">hexo 官方文档</a>、<a href="https://www.runoob.com/markdown/md-tutorial.html">markdown使用教程</a>、<a href="https://butterfly.js.org/">butterfly主题官方文档</a><br>如果看到这里的你也想搭建自己的博客，不妨看看这些资源，也许对你有用！</p><h1>未来</h1><p>在接下来的日子里，本站主要专注于分享自己的学习笔记、项目代码、个人感悟等，另外，我还会继续改善美化本站，尝试更多的魔改方法，尝试更多新鲜有趣的事情！希望读者也能够一起加油！fighting!</p><img src="/2022/03/28/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/2022-03-29-21-10-15.png" class title="This is an example image"> ]]></content>
    
    
    <summary type="html">这是我的第一篇博客，你好！</summary>
    
    
    
    <category term="hexo" scheme="https://yigexiaogai.github.io/categories/hexo/"/>
    
    
    <category term="hexo相关" scheme="https://yigexiaogai.github.io/tags/hexo%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://yigexiaogai.github.io/2022/02/25/hello-world/"/>
    <id>https://yigexiaogai.github.io/2022/02/25/hello-world/</id>
    <published>2022-02-25T11:54:33.968Z</published>
    <updated>2022-02-25T11:54:33.968Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start">Quick Start</h2><h3 id="Create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
